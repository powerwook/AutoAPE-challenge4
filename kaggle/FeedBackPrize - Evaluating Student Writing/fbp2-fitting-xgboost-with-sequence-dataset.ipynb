{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import ast, gc, os, warnings, glob, random, sys, pickle\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.utils.data import Dataset, DataLoader\n","from torch import Tensor\n","from transformers import AutoConfig, AutoModel, AutoTokenizer\n","from collections import OrderedDict\n","from scipy import stats\n","from collections import Counter\n","from bisect import bisect_left\n","from joblib import Parallel, delayed\n","from multiprocessing import Manager\n","from tqdm.auto import tqdm\n","\n","import xgboost as xgb\n","from sklearn.model_selection import cross_val_score, GroupKFold, KFold, train_test_split\n","from sklearn.ensemble import GradientBoostingClassifier\n","from skopt.space import Real\n","from skopt import gp_minimize\n","sys.path.append(\"../input/tez-lib/\")\n","sys.path.append(\"/kaggle/input/acceleratemaster/\")\n","sys.path.append(\"/kaggle/input/acceleratemaster/src\")\n","import src.accelerate \n","import tez\n","\n","warnings.filterwarnings('ignore', '.*ragged nested sequences*')\n","os.environ['TOKENIZERS_PARALLELISM'] = \"false\"\n","os.environ['LRU_CACHE_CAPACITY'] = \"1\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\" \n","Configuration Variable for setting this notebook mode, tuning classifier or inference for submission\n","Variable:\n","    TRAIN_SEQ_CLASSIFIERS: if you want to tune boosting algorithm, set true\n","    submit: if you want to inference for making submission, set true\n","    SUBMISSION: if you want to inference for making submission, set true\n","\"\"\"\n","TRAIN_SEQ_CLASSIFIERS = True\n","submit = False\n","SUBMISSION = False\n","\n","\n","\"\"\" if you set SUBMISSION True, make test dataframe from test.txt \"\"\"\n","if SUBMISSION:\n","    test_names, test_texts = [], []\n","    for f in list(os.listdir('../input/feedback-prize-2021/test')):\n","        test_names.append(f.replace('.txt', ''))\n","        test_texts.append(open('../input/feedback-prize-2021/test/' + f, 'r').read())\n","    test_texts = pd.DataFrame({'id': test_names, 'text': test_texts})"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\" Configuration Class for LLM, Classifier such as XGBoost, LightGBM, CatBoost \"\"\"\n","\n","class CFG1:\n","    \"\"\" Inference Configuration Class for DeBERTa-V3-Large Sequence Length 2048 \"\"\"\n","    wandb = True\n","    seed = 42\n","    n_gpu = 1\n","    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","    gpu_id = 0\n","    num_workers = 0\n","    weight_path = '/kaggle/input/feedbackprize-2-deberta-v3-larrge-baseline-fold5'\n","    model = '/kaggle/input/huggingface-automodel-save/deberta-v3-large/'\n","    reinit = True\n","    tokenizer = AutoTokenizer.from_pretrained(model)\n","    n_folds = 5\n","    max_len = 2048\n","    val_batch_size = 8\n","    xgb_params = {\n","        'learning_rate': 0.05,\n","        'n_estimators': 200,\n","        'max_depth': 7,\n","        'min_child_weight': 5,\n","        'gamma': 0,\n","        'subsample': 0.7,\n","        'reg_alpha': 0.0005,\n","        'colsample_bytree': 0.6,\n","        'scale_pos_weight': 1,\n","        'use_label_encoder': False,\n","        'eval_metric': 'logloss',\n","        'tree_method': 'hist',\n","        'random_state': 42,\n","        'n_jobs': -1,\n","    }\n","\n","# class CFG2:\n","#     \"\"\" Inference Configuration Class for Longformer Sequence Length 4096 \"\"\"\n","#     wandb = True\n","#     seed = 42\n","#     n_gpu = 1\n","#     device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","#     gpu_id = 0\n","#     num_workers = 0\n","#     weight_path = '/kaggle/input/longformer-large-v9'\n","#     model = '/kaggle/input/longformerlarge4096/longformer-large-4096/'\n","#     reinit = True\n","#     tokenizer = AutoTokenizer.from_pretrained(model)\n","#     n_folds = 5\n","#     max_len = 4096\n","#     val_batch_size = 8\n","#     xgb_params = {\n","#         'learning_rate': 0.05,\n","#         'n_estimators': 200,\n","#         'max_depth': 7,\n","#         'min_child_weight': 5,\n","#         'gamma': 0,\n","#         'subsample': 0.7,\n","#         'reg_alpha': 0.0005,\n","#         'colsample_bytree': 0.6,\n","#         'scale_pos_weight': 1,\n","#         'use_label_encoder': False,\n","#         'eval_metric': 'logloss',\n","#         'tree_method': 'hist',\n","#         'random_state': 42,\n","#         'n_jobs': -1,\n","#     }"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\" python & pytorch Reproducility Setting \"\"\"\n","\n","def check_library(checker: bool) -> tuple:\n","    \"\"\"\n","    1) checker == True\n","        - current device is mps\n","    2) checker == False\n","        - current device is cuda with cudnn\n","    \"\"\"\n","    if not checker:\n","        _is_built = torch.backends.cudnn.is_available()\n","        _is_enable = torch.backends.cudnn.enabled\n","        version = torch.backends.cudnn.version()\n","        device = (_is_built, _is_enable, version)\n","        return device\n","\n","def class2dict(cfg) -> dict:\n","    return dict((name, getattr(cfg, name)) for name in dir(cfg) if not name.startswith('__'))\n","\n","def all_type_seed(cfg, checker: bool) -> None:\n","    \"\"\" init seed for python, random, numpy, pytorch \"\"\"\n","    os.environ['PYTHONHASHSEED'] = str(cfg.seed)  # python Seed\n","    random.seed(cfg.seed)  # random module Seed\n","    np.random.seed(cfg.seed)  # numpy module Seed\n","    torch.manual_seed(cfg.seed)  # Pytorch CPU Random Seed Maker\n","\n","    # device == cuda\n","    if not checker:\n","        torch.cuda.manual_seed(cfg.seed)  # Pytorch GPU Random Seed Maker\n","        torch.cuda.manual_seed_all(cfg.seed)  # Pytorch Multi Core GPU Random Seed Maker\n","        # torch.cudnn seed\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = True\n","        torch.backends.cudnn.enabled = True\n","\n","    # devide == mps\n","    else:\n","        torch.mps.manual_seed(cfg.seed)\n","\n","def seed_worker(worker_id) -> None:\n","    \"\"\" init seed for numpy, random library \"\"\"\n","    worker_seed = torch.initial_seed() % 2 ** 32\n","    np.random.seed(worker_seed)\n","    random.seed(worker_seed)\n","\n","print(check_library(False))\n","all_type_seed(CFG2, True)\n","g = torch.Generator()\n","g.manual_seed(CFG2.seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\" Data Preprocessing Utils \"\"\"\n","\n","def ner_tokenizing(cfg, text: str):\n","    \"\"\"\n","    Preprocess text for NER Pipeline\n","    if you want to set param 'return_offsets_mapping' == True, you must use FastTokenizer\n","    you must use PretrainedTokenizer which is supported FastTokenizer\n","    Converting text to torch.Tensor will be done in Custom Dataset Class\n","    Params:\n","        return_offsets_mapping:\n","            - bool, defaults to False\n","            - Whether or not to return (char_start, char_end) for each token.\n","            => useful for NER Task\n","    Args:\n","        cfg: configuration.CFG, needed to load tokenizer from Huggingface AutoTokenizer\n","        text: text from dataframe or any other dataset, please pass str type\n","    \"\"\"\n","    inputs = cfg.tokenizer(\n","        text,\n","        return_offsets_mapping=True,  # only available for FastTokenizer by Rust, not erase /n, /n/n\n","        max_length=cfg.max_len,\n","        padding='max_length',\n","        truncation=True,\n","        return_tensors=None,\n","        add_special_tokens=True,\n","    )\n","    return inputs\n","\n","def load_data(data_path: str) -> pd.DataFrame:\n","    \"\"\"\n","    Load data_folder from csv file like as train.csv, test.csv, val.csv\n","    \"\"\"\n","    df = pd.read_csv(data_path)\n","    return df\n","\n","def labels2ids():\n","    \"\"\"\n","    Encoding labels to ids for neural network with BIO Styles\n","    labels2dict = {\n","    'O': 0, 'B-Lead': 1, 'I-Lead': 2, 'B-Position': 3, 'I-Position': 4, 'B-Claim': 5,\n","    'I-Claim': 6, 'B-Counterclaim': 7, 'I-Counterclaim': 8, 'B-Rebuttal': 9, 'I-Rebuttal': 10,\n","    'B-Evidence': 11, 'I-Evidence': 12, 'B-Concluding Statement': 13, 'I-Concluding Statement': 14\n","     }\n","    \"\"\"\n","    output_labels = [\n","        'O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim',\n","        'I-Counterclaim', 'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement',\n","        'I-Concluding Statement'\n","    ]\n","    labels_to_ids = {v: k for k, v in enumerate(output_labels)}\n","    return labels_to_ids\n","\n","\n","def ids2labels():\n","    \"\"\"\n","    Decoding labels to ids for neural network with BIO Styles\n","    labels2dict = {\n","    'O': 0, 'B-Lead': 1, 'I-Lead': 2, 'B-Position': 3, 'I-Position': 4, 'B-Claim': 5,\n","    'I-Claim': 6, 'B-Counterclaim': 7, 'I-Counterclaim': 8, 'B-Rebuttal': 9, 'I-Rebuttal': 10,\n","    'B-Evidence': 11, 'I-Evidence': 12, 'B-Concluding Statement': 13, 'I-Concluding Statement': 14\n","     }\n","    \"\"\"\n","    output_labels = [\n","        'O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim',\n","        'I-Counterclaim', 'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement',\n","        'I-Concluding Statement'\n","    ]\n","    ids_to_labels = {k: v for k, v in enumerate(output_labels)}\n","    return ids_to_labels\n","\n","\n","def split_mapping(unsplit):\n","    \"\"\" Return array which is mapping character index to index of word in list of split() words \"\"\"\n","    splt = unsplit.split()\n","    offset_to_wordidx = np.full(len(unsplit), -1)\n","    txt_ptr = 0\n","    for split_index, full_word in enumerate(splt):\n","        while unsplit[txt_ptr:txt_ptr + len(full_word)] != full_word:\n","            txt_ptr += 1\n","        offset_to_wordidx[txt_ptr:txt_ptr + len(full_word)] = split_index\n","        txt_ptr += len(full_word)\n","    return offset_to_wordidx\n","\n","\n","# class Collate:\n","#     def __init__(self, tokenizer):\n","#         self.tokenizer = tokenizer\n","\n","#     def __call__(self, batch):\n","#         output = dict()\n","#         output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n","#         output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n","\n","#         # calculate max token length of this batch\n","#         batch_max = max([len(ids) for ids in output[\"ids\"]])\n","\n","#         # add padding\n","#         if self.tokenizer.padding_side == \"right\":\n","#             output[\"ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"ids\"]]\n","#             output[\"mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"mask\"]]\n","#         else:\n","#             output[\"ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"ids\"]]\n","#             output[\"mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"mask\"]]\n","\n","#         # convert to tensors\n","#         output[\"ids\"] = torch.tensor(output[\"ids\"], dtype=torch.long)\n","#         output[\"mask\"] = torch.tensor(output[\"mask\"], dtype=torch.long)\n","\n","#         return output\n","\n","    \n","# def _prepare_test_data_helper(args, tokenizer, ids):\n","#     test_samples = []\n","#     for idx in ids:\n","#         filename = os.path.join(args.input_path, \"test\", idx + \".txt\")\n","#         with open(filename, \"r\") as f:\n","#             text = f.read()\n","\n","#         encoded_text = tokenizer.encode_plus(\n","#             text,\n","#             add_special_tokens=False,\n","#             return_offsets_mapping=True,\n","#         )\n","#         input_ids = encoded_text[\"input_ids\"]\n","#         offset_mapping = encoded_text[\"offset_mapping\"]\n","\n","#         sample = {\n","#             \"id\": idx,\n","#             \"input_ids\": input_ids,\n","#             \"text\": text,\n","#             \"offset_mapping\": offset_mapping,\n","#         }\n","\n","#         test_samples.append(sample)\n","#     return test_samples\n","\n","\n","# def prepare_test_data(df, tokenizer, args):\n","#     test_samples = []\n","#     ids = df[\"id\"].unique()\n","#     ids_splits = np.array_split(ids, 4)\n","\n","#     results = Parallel(n_jobs=4, backend=\"multiprocessing\")(\n","#         delayed(_prepare_test_data_helper)(args, tokenizer, idx) for idx in ids_splits\n","#     )\n","#     for result in results:\n","#         test_samples.extend(result)\n","\n","#     return test_samples"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\" Trainer Utils & Metric Function \"\"\" \n","\n","def get_name(cfg) -> str:\n","    \"\"\" get name of model \"\"\"\n","    try:\n","        name = cfg.model.replace('/', '-')\n","    except ValueError:\n","        name = cfg.model\n","    return name\n","\n","def calc_overlap(row):\n","    \"\"\"\n","    Calculates the overlap between prediction and\n","    ground truth and overlap percentages used for determining\n","    true positives.\n","    \"\"\"\n","    set_pred = set(row.predictionstring_pred.split(' '))\n","    set_gt = set(row.predictionstring_gt.split(' '))\n","    # Length of each and intersection\n","    len_gt = len(set_gt)\n","    len_pred = len(set_pred)\n","    inter = len(set_gt.intersection(set_pred))\n","    overlap_1 = inter / len_gt\n","    overlap_2 = inter / len_pred\n","    return [overlap_1, overlap_2]\n","\n","\n","def calculate_f1(pred_df: pd.DataFrame, gt_df: pd.DataFrame) -> float:\n","    \"\"\"\n","    Function for scoring for competition\n","    Step 1:\n","        Make dataframe all ground truths and predictions for a given class are compared\n","    Step 2:\n","        If the overlap between the ground truth and prediction is >= 0.5 (Recall),\n","        and the overlap between the prediction and the ground truth >= 0.5 (Precision),\n","        In other words, prediction will be accepted 'True Positive',\n","        when Precision & Recall greater than 0.5\n","        the prediction is a match and considered a true positive.\n","        If multiple matches exist, the match with the highest pair of overlaps is taken.\n","        And then count number of Potential True Positive ids\n","    Step 3:\n","        Any unmatched ground truths are false negatives and any unmatched predictions are false positives.\n","        And then count number of Potential False Positives\n","    Step 4.\n","        Calculate Micro F1-Score for Cross Validation\n","    Reference:\n","        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n","    \"\"\"\n","    gt_df = gt_df[['id', 'discourse_type', 'predictionstring']].reset_index(drop=True).copy()\n","    pred_df = pred_df[['id', 'class', 'predictionstring']].reset_index(drop=True).copy()\n","    pred_df['pred_id'] = pred_df.index\n","    gt_df['gt_id'] = gt_df.index\n","    # Step 1. all ground truths and predictions for a given class are compared.\n","    joined = pred_df.merge(gt_df,\n","                           left_on=['id', 'class'],\n","                           right_on=['id', 'discourse_type'],\n","                           how='outer',\n","                           suffixes=('_pred', '_gt')\n","                           )\n","    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n","    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n","\n","    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n","\n","    # 2. If the overlap between the ground truth and prediction is >= 0.5,\n","    # and the overlap between the prediction and the ground truth >= 0.5,\n","    # the prediction is a match and considered a true positive.\n","    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n","    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n","    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n","\n","    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n","    joined['max_overlap'] = joined[['overlap1', 'overlap2']].max(axis=1)\n","    tp_pred_ids = joined.query('potential_TP') \\\n","        .sort_values('max_overlap', ascending=False) \\\n","        .groupby(['id', 'predictionstring_gt']).first()['pred_id'].values\n","\n","    # 3. Any unmatched ground truths are false negatives\n","    # and any unmatched predictions are false positives.\n","    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n","\n","    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n","    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n","\n","    # Get numbers of each type\n","    TP = len(tp_pred_ids)\n","    FP = len(fp_pred_ids)\n","    FN = len(unmatched_gt_ids)\n","    # calc microf1\n","    my_f1_score = TP / (TP + 0.5 * (FP + FN))\n","    return my_f1_score"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\" Custom Dataset Class \"\"\"\n","\n","class NERDataset(Dataset):\n","    \"\"\"\n","    Custom Dataset Class for NER Task\n","    Args:\n","        cfg: configuration.CFG\n","        df: dataframe from .txt file\n","        is_train: if this param set False, return word_ids from self.df.entities\n","    \"\"\"\n","    def __init__(self, cfg: CFG2, df: pd.DataFrame, is_train: bool = True) -> None:\n","        self.cfg = cfg\n","        self.df = df\n","        self.tokenizer = ner_tokenizing\n","        self.labels2ids = labels2ids()  # Function for Encoding Labels to ids\n","        self.ids2labels = ids2labels()  # Function for Decoding ids to Labels\n","        self.is_train = is_train\n","\n","    def __len__(self) -> int:\n","        return len(self.df)\n","\n","    def __getitem__(self, item: int) -> tuple[list, [dict[Tensor, Tensor, Tensor], Tensor]]:\n","        \"\"\"\n","        1) Tokenizing input text:\n","            - if you param 'return_offsets_mapping' == True, tokenizer doen't erase \\n or \\n\\n\n","              but, I don't know this param also applying for DeBERTa Pretrained Tokenizer\n","        2) Create targets and mapping of tokens to split() words by tokenizer\n","            - Mapping Labels to split tokens\n","            - Iterate in reverse to label whitespace tokens until a Begin token is encountered\n","            - Tokenizer will split word into subsequent of character such as copied => copy, ##ed\n","            - So, we need to find having same parent token and then label BIO NER Tags\n","        3) Return dict:\n","            - Train: dict.keys = [inputs_id, attention_mask, token_type_ids, labels]\n","            - Validation/Test: dict.keys = [inputs_id, attention_mask, token_type_ids, word_ids]\n","        \"\"\"\n","        ids = self.df.id[item]\n","        text = self.df.text[item]\n","        if self.is_train:\n","            word_labels = ast.literal_eval(self.df.entities[item])\n","\n","        # 1) Tokenizing input text\n","        encoding = self.tokenizer(\n","            self.cfg,\n","            text,\n","        )\n","        word_ids = encoding.word_ids()\n","        split_word_ids = np.full(len(word_ids), -1)\n","        offset_to_wordidx = split_mapping(text)  # [1, sequence_length]\n","        offsets = encoding['offset_mapping']  # [(src, end), (src, end), ...]\n","\n","        # 2) Find having same parent token and then label BIO NER Tags\n","        label_ids = []\n","        for token_idx, word_idx in reversed(list(enumerate(word_ids))):\n","            if word_idx is None:\n","                \"\"\" for padding token \"\"\"\n","                if self.is_train:\n","                    label_ids.append(-100)\n","            else:\n","                if offsets[token_idx] != (0, 0):\n","                    # Choose the split word that shares the most characters with the token if any\n","                    split_idxs = offset_to_wordidx[offsets[token_idx][0]:offsets[token_idx][1]]\n","                    split_index = stats.mode(split_idxs[split_idxs != -1]).mode[0] if len(\n","                        np.unique(split_idxs)) > 1 else split_idxs[0]\n","                    if split_index != -1:\n","                        if self.is_train:\n","                            label_ids.append(self.labels2ids[word_labels[split_index]])\n","                        split_word_ids[token_idx] = split_index\n","                    else:\n","                        # Even if we don't find a word, continue labeling 'I' tokens until a 'B' token is found\n","                        if label_ids and label_ids[-1] != -100 and self.ids2labels[label_ids[-1]][0] == 'I':\n","                            split_word_ids[token_idx] = split_word_ids[token_idx + 1]\n","                            if self.is_train:\n","                                label_ids.append(label_ids[-1])\n","                        else:\n","                            if self.is_train:\n","                                label_ids.append(-100)\n","                else:\n","                    if self.is_train:\n","                        label_ids.append(-100)\n","        if not self.is_train:\n","            encoding['word_ids'] = torch.as_tensor(split_word_ids)\n","        else:\n","            encoding['labels'] = list(reversed(label_ids))\n","        for k, v in encoding.items():\n","            encoding[k] = torch.as_tensor(v)\n","        return ids, encoding\n","    \n","    \n","# class LongformerNERDataset:\n","#     \"\"\" Custom Dataset Class for Longformer \"\"\"\n","#     def __init__(self, samples, max_len, tokenizer):\n","#         self.samples = samples\n","#         self.max_len = max_len\n","#         self.tokenizer = tokenizer\n","#         self.length = len(samples)\n","\n","#     def __len__(self):\n","#         return self.length\n","\n","#     def __getitem__(self, idx):\n","#         input_ids = self.samples[idx][\"input_ids\"]\n","#         # print(input_ids)\n","#         # print(input_labels)\n","\n","#         # add start token id to the input_ids\n","#         input_ids = [self.tokenizer.cls_token_id] + input_ids\n","\n","#         if len(input_ids) > self.max_len - 1:\n","#             input_ids = input_ids[: self.max_len - 1]\n","\n","#         # add end token id to the input_ids\n","#         input_ids = input_ids + [self.tokenizer.sep_token_id]\n","#         attention_mask = [1] * len(input_ids)\n","\n","#         # padding_length = self.max_len - len(input_ids)\n","#         # if padding_length > 0:\n","#         #     if self.tokenizer.padding_side == \"right\":\n","#         #         input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length\n","#         #         attention_mask = attention_mask + [0] * padding_length\n","#         #     else:\n","#         #         input_ids = [self.tokenizer.pad_token_id] * padding_length + input_ids\n","#         #         attention_mask = [0] * padding_length + attention_mask\n","\n","#         # return {\n","#         #     \"ids\": torch.tensor(input_ids, dtype=torch.long),\n","#         #     \"mask\": torch.tensor(attention_mask, dtype=torch.long),\n","#         # }\n","\n","#         return {\n","#             \"ids\": input_ids,\n","#             \"mask\": attention_mask,\n","#         }"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\" Custom Model Class \"\"\"\n","\n","class DeBERTaModel(nn.Module):\n","    \"\"\"\n","    Model class For NER Task Pipeline, in this class no pooling layer with backbone named \"DeBERTa\"\n","    This pipeline apply B.I.O Style, so the number of classes is 15 which is 7 unique classes original\n","    Each of 7 unique classes has sub 2 classes (B, I) => 14 classes\n","    And 1 class for O => 1 class\n","    14 + 1 = 15 classes\n","    Args:\n","        cfg: configuration.CFG\n","    \"\"\"\n","    def __init__(self, cfg) -> None:\n","        super().__init__()\n","        self.cfg = cfg\n","        self.auto_cfg = AutoConfig.from_pretrained(\n","            self.cfg.model,\n","            output_hidden_states=True\n","        )\n","        self.model = AutoModel.from_pretrained(\n","            self.cfg.model,\n","            config=self.auto_cfg\n","        )\n","        self.fc = nn.Linear(self.auto_cfg.hidden_size, 15)  # BIO Style NER Task\n","\n","    def feature(self, inputs_ids, attention_mask, token_type_ids):\n","        outputs = self.model(\n","            input_ids=inputs_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids\n","        )\n","        return outputs\n","\n","    def forward(self, inputs) -> Tensor:\n","        \"\"\"\n","        No Pooling Layer for word-level task\n","        Args:\n","            inputs: Dict type from AutoTokenizer\n","            => {input_ids, attention_mask, token_type_ids, offset_mapping, labels}\n","        \"\"\"\n","        outputs = self.feature(\n","            inputs_ids=inputs[\"input_ids\"],\n","            attention_mask=inputs[\"attention_mask\"],\n","            token_type_ids=inputs[\"token_type_ids\"],\n","        )\n","        logit = self.fc(outputs.last_hidden_state)\n","        return logit\n","\n","class LongformerModel(nn.Module):\n","    \"\"\" Model class for fine-tuned longformer \"\"\"\n","    def __init__(self, cfg):\n","        super(LongformerModel, self).__init__()\n","        self.cfg = cfg\n","        self.auto_cfg = AutoConfig.from_pretrained(\n","            self.cfg.model,\n","        )\n","        self.model = AutoModel.from_pretrained(\n","            self.cfg.model,\n","        )\n","        self.output = nn.Linear(self.auto_cfg.hidden_size, 15)\n","        self.drop_out = nn.Dropout(0.1)\n","        self.dropout1 = nn.Dropout(0.1)\n","        self.dropout2 = nn.Dropout(0.2)\n","        self.dropout3 = nn.Dropout(0.3)\n","        self.dropout4 = nn.Dropout(0.4)\n","        self.dropout5 = nn.Dropout(0.5)\n","        \n","    def feature(self, inputs_ids, attention_mask, token_type_ids):\n","        outputs = self.model(\n","            input_ids=inputs_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids\n","        )\n","        return outputs\n","\n","    def forward(self, inputs) -> Tensor:\n","        \"\"\"\n","        No Pooling Layer for word-level task\n","        Args:\n","            inputs: Dict type from AutoTokenizer\n","            => {input_ids, attention_mask, token_type_ids, offset_mapping, labels}\n","        \"\"\"\n","        outputs = self.feature(\n","            inputs_ids=inputs[\"input_ids\"],\n","            attention_mask=inputs[\"attention_mask\"],\n","            token_type_ids=inputs[\"token_type_ids\"],\n","        ).last_hidden_state\n","        \n","        preds1 = self.output(self.dropout1(outputs))\n","        preds2 = self.output(self.dropout2(outputs))\n","        preds3 = self.output(self.dropout3(outputs))\n","        preds4 = self.output(self.dropout4(outputs))\n","        preds5 = self.output(self.dropout5(outputs))\n","        preds = (preds1 + preds2 + preds3 + preds4 + preds5) / 5\n","        return preds"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\" torch.cuda, cudnn, reproducibility setting \"\"\"\n","\n","check_library(True)\n","all_type_seed(CFG2, True)\n","g = torch.Generator()\n","g.manual_seed(CFG2.seed)\n","\n","\n","\"\"\" Trainer Class for Make Sequence Dataset for Multiple Label Classification Task Pipeline \"\"\"\n","\n","class SequenceDataTrainer:\n","    \"\"\"\n","    Only Forward Pass with Validation Dataset for Making Sequence Dataset by whole Competition Data\n","    This Trainer Class for DeBERTa model\n","    \"\"\"\n","    def __init__(self, cfg, df: pd.DataFrame, generator: torch.Generator) -> None:\n","        self.cfg = cfg\n","        self.model_name = get_name(self.cfg)\n","        self.generator = generator\n","        self.df = df\n","\n","    def make_batch(self, fold: int) -> tuple[torch.utils.data.DataLoader, pd.DataFrame]:\n","        \"\"\"\n","        Make Batch Dataset for main train loop\n","        Select 50% Dataset of each fold randomly\n","        Using Fold 0 for tuning XGBoost\n","        \"\"\"\n","        # Custom Datasets\n","        valid_dataset = NERDataset(self.cfg, self.df, is_train=False)\n","        loader_valid = DataLoader(\n","            valid_dataset,\n","            batch_size=self.cfg.val_batch_size,\n","            shuffle=False,\n","            worker_init_fn=seed_worker,\n","            generator=self.generator,\n","            num_workers=self.cfg.num_workers,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","        return loader_valid, self.df\n","\n","    def model_setting(self, model_name: str, path: str, fold: int):\n","        \"\"\" load fine-tuned model's weight, iterate by fold \"\"\"\n","        if model_name == 'deberta':\n","            model = DeBERTaModel(self.cfg)\n","            model.load_state_dict(torch.load(path, map_location=\"cuda:0\"),)\n","        else:\n","            model = LongformerModel(\n","                self.cfg\n","            )\n","#             model = torch.nn.DataParallel(model)\n","#             model.load_state_dict(torch.load(path, map_location=\"cuda:0\"),)\n","            longformer_state_dict = torch.load(path, map_location=\"cuda:0\")\n","            new_state_dict = OrderedDict()\n","            for k, v in longformer_state_dict.items():\n","                name = k[7:]  # remove `module.`\n","                new_state_dict[name] = v\n","            model.load_state_dict(new_state_dict, strict=False)  # strict=False\n","            \n","        model.to(self.cfg.device)\n","        return model\n","\n","    def inference_fn(self, loader_valid: torch.utils.data.DataLoader, model: nn.Module) -> tuple[list, list]:\n","        \"\"\"\n","        Validation Functions\n","        Not convert probability to string label text with torch.argmax\n","        function should return those shape of Tensor: [batch_size, sequence_length, num_labels] == outputs.last_hidden_state\n","        Variable:\n","            val_ids_list: list of ids for calculating sequence dataset\n","        \"\"\"\n","        val_ids_list = []\n","        full_pred = np.zeros((len(loader_valid.dataset), self.cfg.max_len, 15), dtype=np.float32)\n","        word_ids = np.full((len(loader_valid.dataset), self.cfg.max_len), -100)\n","        model.eval()\n","        with torch.no_grad():\n","            for step, (ids, inputs) in enumerate(tqdm(loader_valid)): \n","                # inputs = collate(inputs)\n","                for k, v in inputs.items():\n","                    inputs[k] = v.to(self.cfg.device)  # prompt to GPU\n","\n","                val_pred = model(inputs)  # [batch_size, sequence_length, num_labels]\n","                val_prob = F.softmax(val_pred, dim=2).cpu().detach().numpy()  # dim 2 == num_labels dim\n","                full_pred[step*self.cfg.val_batch_size:(step+1)*self.cfg.val_batch_size] += val_prob\n","                word_ids[step*self.cfg.val_batch_size:(step+1)*self.cfg.val_batch_size] = inputs[\"word_ids\"].cpu().detach().numpy()\n","                val_ids_list.extend(ids)\n","                \n","            # 2) make prediction list\n","            predictions = []\n","            for idx in range(full_pred.shape[0]):\n","                \"\"\" loop for each unique ids \"\"\"\n","                prediction, prob_buffer, previous_word_idx = [], [], -1\n","                sequence_logit = full_pred[idx]\n","                sub_ids = word_ids[idx][word_ids[idx] != -100]\n","                for i, word_idx in enumerate(sub_ids):\n","                    if word_idx == -1:\n","                        pass\n","                    elif word_idx != previous_word_idx:\n","                        if prob_buffer:\n","                            prediction.append(np.mean(prob_buffer, dtype=np.float32, axis=0))\n","                            prob_buffer = []\n","                        prob_buffer.append(sequence_logit[i])\n","                        previous_word_idx = word_idx\n","                    else:\n","                        prob_buffer.append(sequence_logit[i])\n","                prediction.append(np.mean(prob_buffer, dtype=np.float32, axis=0))\n","                predictions.append(prediction)\n","            gc.collect()\n","            torch.cuda.empty_cache()\n","        return val_ids_list, predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\" Sorting OOF DeBERTa-V3-Large Model weight list \"\"\"\n","\n","sorted_model_list = []\n","model_list = glob.glob(f'{CFG1.weight_path}/*.pth')\n","for idx in range(len(model_list)):\n","    num = model_list[idx].split('/')[4][4]\n","    sorted_model_list.append([model_list[idx], num])\n","sorted_model_list.sort(key=lambda x:x[1])\n","\n","for idx in range(len(model_list)):\n","    model_list[idx] = sorted_model_list[idx][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\"\n","DeBERTa-V3-Large Inference\n","\"\"\"\n","tmp, model_name = 0, 'deberta'\n","all_id_list, all_pred_list = [], []\n","tmp_dataframe = load_data('/kaggle/input/fbp2-preprocessed-train-dataframe/final_converted_train_df.csv')\n","random.seed(42)\n","sample_id_list = random.sample(tmp_dataframe.id.to_list(), int(len(tmp_dataframe) * 0.5))\n","tmp_dataframe = tmp_dataframe[tmp_dataframe['id'].isin(sample_id_list) == True].reset_index(drop=True)\n","\n","for fold, model_path in tqdm(enumerate(model_list)):\n","    print(f'============== {fold}th Fold forward ==============')\n","    forward_input = SequenceDataTrainer(CFG1, tmp_dataframe, g)\n","    loader_valid, valid = forward_input.make_batch(fold)\n","    fold_model = forward_input.model_setting(model_name, model_path, fold)\n","    # forward pass\n","    all_id_list, predictions = forward_input.inference_fn(loader_valid, fold_model)\n","    all_pred_list.append(predictions)\n","    del fold_model\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","\"\"\" OOF 5 Folds \"\"\"\n","for i in range(len(all_pred_list)):\n","    for j in range(len(all_pred_list[i])):\n","        tmp += all_pred_list[i][j]\n","deberta_oof = tmp / 5"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\"\n","Let's Start Making Sequence Dataset by forwarding each fold's dataset to fold's model weight\n","This loop function for pred list which is used to making sequence dataframe\n","We can tune 3 Boosting Algorithm with \"Full Train Dataset\"\n","This code aim to use full train data but, due to time out problem, we can't use all of them\n","So, We will use 50% of each fold first and then increase amount of dataset\n","\"\"\"\n","# model_name = 'longformer'\n","# longformer_all_id_list, longformer_all_pred_list = [], []\n","# model_list = glob.glob(f'{CFG2.weight_path}/*.pt')\n","# for fold, model_path in tqdm(enumerate(model_list)):\n","#     print(f'============== {fold}th fold forward ==============')\n","#     forward_input = SequenceDataTrainer(CFG2, g)\n","#     loader_valid, valid = forward_input.make_batch(fold)\n","#     fold_model = forward_input.model_setting(model_name, model_path, fold)\n","#     # forward pass\n","#     longformer_all_id_list, predictions = forward_input.inference_fn(loader_valid, fold_model)\n","#     longformer_all_pred_list.append(predictions)\n","#     del fold_model\n","#     gc.collect()\n","#     torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\" Save prediction array for speed up debugging & tuning \"\"\"\n","\n","\"\"\" DeBERTa-V3-Large \"\"\"\n","np.save('sampling_pred_list.npy', deberta_oof)  # save predictions\n","np.save('sampling_id_list.npy', np.array(sample_id_list))  # save id list\n","\n","# \"\"\" Longformer-4096-Large \"\"\"\n","# longformer_sampling_predictions = np.array(longformer_all_pred_list)\n","# np.save('longformer_sampling_pred_list.npy', longformer_sampling_predictions)  # save predictions\n","# np.save('longformer_sampling_id_list.npy', np.array(longformer_all_id_list))  # save id list"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# \"\"\" \n","# 1) Load DeBERTa-V3-Large, Longformer-Large-4096 inference array\n","# 2) Ensemble Each inference Result\n","# \"\"\"\n","\n","# \"\"\"\n","# Load prediction result for Sampling train dataframe from DeBERTa-V3-Large\n","# \"\"\"\n","# deberta_all_pred_list = np.load(\n","#     \"/kaggle/input/feedbackprize2-sampling-inference-for-tuning/sampling_pred_list.npy\",\n","#     allow_pickle=True\n","# )\n","# deberta_all_id_list = np.load(\n","#     '/kaggle/input/feedbackprize2-sampling-inference-for-tuning/sampling_id_list.npy',\n","#     allow_pickle=True\n","# )\n","\n","# \"\"\"\n","# Load prediction result for Sampling train dataframe from Longformer-Large-4096\n","# \"\"\"\n","# longformer_all_pred_list = np.load(\n","#     \"/kaggle/input/feedbackprize2-sampling-inference-for-tuning/sampling_pred_list.npy\",\n","#     allow_pickle=True\n","# )\n","# lonformer_all_id_list = np.load(\n","#     '/kaggle/input/feedbackprize2-sampling-inference-for-tuning/sampling_id_list.npy',\n","#     allow_pickle=True\n","# )\n","\n","\"\"\" Ensemble Two Inference Result for Tuning XGBoost \"\"\"\n","\n","uniqueValidGroups = range(len(all_pred_list))\n","uniqueValidGroups"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\"\n","Convert pred list into Sequence DataFrame\n","This code aim to use full train data but, due to time out problem, we can't use all of them\n","So, We will use 10% data of each fold first and then increase if it will be fine\n","\"\"\"\n","# 1) Make Input\n","train_df = load_data('/kaggle/input/feedback-prize-2021/train.csv')\n","valid_df = train_df[train_df['id'].isin(all_id_list) == True].reset_index(drop=True)\n","\n","disc_type_to_ids = {\n","    'Evidence':(11,12),\n","    'Claim':(5,6),\n","    'Lead':(1,2),\n","    'Position':(3,4),\n","    'Counterclaim':(7,8),\n","    'Rebuttal':(9,10),\n","    'Concluding Statement':(13,14)\n","}\n","\n","# 2) Minimum Threshold Value of Each Target Classes, Reference from Discussion from competition\n","MIN_BEGIN_PROB = {\n","    'Claim': .35,\n","    'Concluding Statement': .15,\n","    'Counterclaim': .04,\n","    'Evidence': .1,\n","    'Lead': .32,\n","    'Position': .25,\n","    'Rebuttal': .01,\n","}\n","\n","MAX_SEQ_LEN = {}\n","train_df['len'] = train_df['predictionstring'].apply(lambda x:len(x.split()))\n","max_lens = train_df.groupby('discourse_type')['len'].quantile(.995)\n","for disc_type in disc_type_to_ids:\n","    MAX_SEQ_LEN[disc_type] = int(max_lens[disc_type])\n","\n","# 3) Custom Dataset Class\n","class SeqDataset(object):\n","    \"\"\"\n","    Args:\n","        features: sequence length, position, and various kinds of class probability\n","        labels: whether the sequence matches exactly a discourse instance\n","        truePos: whether the sequence matches a discourse instance by competition criteria for true positive\n","        groups: the integer index of the text where the sequence is found\n","        wordRanges: the start and end word index of the sequence in the text\n","    Reference:\n","        https://www.kaggle.com/code/chasembowers/sequence-postprocessing-v2-67-lb\n","    \"\"\"\n","    def __init__(self, features: list, labels: list, groups: list, wordRanges: list, truePos: list) -> None:\n","        self.features = np.array(features, dtype=np.float32)\n","        self.labels = np.array(labels)\n","        self.groups = np.array(groups, dtype=np.int16)\n","        self.wordRanges = np.array(wordRanges, dtype=np.int16)\n","        self.truePos = np.array(truePos)\n","\n","# 4) Making DataFrame Utils Function\n","def sorted_quantile(array: list, q: float):\n","    \"\"\"\n","    This is used to prevent re-sorting to compute quantile for every sequence.\n","    Args:\n","        array: list of element\n","        q: accumulate probability which you want to calculate spot\n","    Reference:\n","        https://stackoverflow.com/questions/60467081/linear-interpolation-in-numpy-quantile\n","        https://www.kaggle.com/code/chasembowers/sequence-postprocessing-v2-67-lb/notebook\n","    \"\"\"\n","    array = np.array(array)\n","    n = len(array)\n","    index = (n - 1) * q\n","    left = np.floor(index).astype(int)\n","    fraction = index - left\n","    right = left\n","    right = right + (fraction > 0).astype(int)\n","    i, j = array[left], array[right]\n","    return i + (j - i) * fraction\n","\n","def sequence_dataset(\n","    disc_type: str,\n","    valid_word_preds: np.ndarray,\n","    dataframe: pd.DataFrame,\n","    test_word_preds: np.ndarray = None,\n","    pred_indices: bool = None,\n","    submit: bool = False\n","        ):\n","    \"\"\"\n","    Function for making sequence dataset for changing NER Task to Multi-Class Classification Task\n","    Args:\n","        disc_type: discourse type, for example 'Claim', 'Evidence' later turned into target classes\n","        valid_word_preds: valid word predictions from neural network which is trained NER Task\n","        dataframe: train dataframe\n","        test_word_preds: test word predictions from neural network which is trained NER Task\n","        pred_indices: indices of valid word predictions\n","        submit: if True, use test_word_preds instead of valid_word_preds\n","    Reference:\n","        https://www.kaggle.com/code/chasembowers/sequence-postprocessing-v2-67-lb/notebook\n","    \"\"\"\n","    word_preds = valid_word_preds if not submit else test_word_preds\n","    window = pred_indices if pred_indices else range(len(word_preds))\n","    X = np.empty((int(1e6),13), dtype=np.float32)\n","    X_ind = 0\n","    y = []\n","    truePos = []\n","    wordRanges = []\n","    groups = []\n","    for text_i in tqdm(window):\n","        text_preds = np.array(word_preds[text_i])\n","        num_words = len(text_preds)\n","        disc_begin, disc_inside = disc_type_to_ids[disc_type]\n","\n","        # The probability that a word corresponds to either a 'B'-egin or 'I'-nside token for a class\n","        prob_or = lambda word_preds: (1-(1-word_preds[:,disc_begin]) * (1-word_preds[:,disc_inside]))\n","\n","        if not submit:\n","            gt_idx = set()\n","            gt_arr = np.zeros(num_words, dtype=int)\n","#             text_gt = valid.loc[valid.id == dataframe.id.values[text_i]]\n","            text_gt = dataframe\n","            disc_gt = text_gt.loc[text_gt.discourse_type == disc_type]\n","\n","            # Represent the discourse instance locations in a hash set and an integer array for speed\n","            for row_i, row in enumerate(disc_gt.iterrows()):\n","                splt = row[1]['predictionstring'].split()\n","                start, end = int(splt[0]), int(splt[-1]) + 1\n","                gt_idx.add((start, end))\n","                gt_arr[start:end] = row_i + 1\n","            gt_lens = np.bincount(gt_arr)\n","\n","        # Iterate over every sub-sequence in the text\n","        quants = np.linspace(0,1,7)  # for quantile\n","        prob_begins = np.copy(text_preds[:,disc_begin])\n","        min_begin = MIN_BEGIN_PROB[disc_type]\n","        for pred_start in range(num_words):\n","            prob_begin = prob_begins[pred_start]\n","            if prob_begin > min_begin:\n","                begin_or_inside = []\n","                for pred_end in range(pred_start+1,min(num_words+1, pred_start+MAX_SEQ_LEN[disc_type]+1)):\n","\n","                    new_prob = prob_or(text_preds[pred_end-1:pred_end])\n","                    insert_i = bisect_left(begin_or_inside, new_prob)\n","                    begin_or_inside.insert(insert_i, new_prob[0])\n","\n","                    # Generate features for a word sub-sequence\n","\n","                    # The length and position of start/end of the sequence\n","                    features = [pred_end - pred_start, pred_start / float(num_words), pred_end / float(num_words)]\n","\n","                    # 7 evenly spaced quantiles of the distribution of relevant class probabilities for this sequence\n","                    features.extend(list(sorted_quantile(begin_or_inside, quants)))\n","\n","                    # The probability that words on either edge of the current sub-sequence belong to the class of interest\n","                    features.append(prob_or(text_preds[pred_start-1:pred_start])[0] if pred_start > 0 else 0)\n","                    features.append(prob_or(text_preds[pred_end:pred_end+1])[0] if pred_end < num_words else 0)\n","\n","                    # The probability that the first word corresponds to a 'B'-egin token\n","                    features.append(text_preds[pred_start,disc_begin])\n","\n","                    exact_match = (pred_start, pred_end) in gt_idx if not submit else None\n","\n","                    if not submit:\n","                        true_pos = False\n","                        for match_cand, count in Counter(gt_arr[pred_start:pred_end]).most_common(2):\n","                            if match_cand != 0 and count / float(pred_end - pred_start) >= .5 and float(count) / gt_lens[match_cand] >= .5: true_pos = True\n","                    else:\n","                        true_pos = None\n","\n","                    # For efficiency, use a numpy array instead of a list that doubles in size when full to conserve constant \"append\" time complexity\n","                    if X_ind >= X.shape[0]:\n","                        new_X = np.empty((X.shape[0]*2,13), dtype=np.float32)\n","                        new_X[:X.shape[0]] = X\n","                        X = new_X\n","                    X[X_ind] = features\n","                    X_ind += 1\n","\n","                    y.append(exact_match)\n","                    truePos.append(true_pos)\n","                    wordRanges.append((np.int16(pred_start), np.int16(pred_end)))\n","                    groups.append(np.int16(text_i))\n","\n","    return SeqDataset(X[:X_ind], y, groups, wordRanges, truePos)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\"\n","Parallelize part sequence dataset generation for Tuning 3 Boosting Algorithm\n","Source Code from:\n","    https://www.kaggle.com/code/chasembowers/sequence-postprocessing-v2-67-lb\n","\"\"\"\n","\n","submit, SUBMISSION = False, False\n","manager = Manager()\n","\n","def generate_sequence_dataset(disc_type: str, submit=False):\n","    if not submit:\n","        if not SUBMISSION:\n","            validSeqSets[disc_type] = sequence_dataset(\n","                disc_type,\n","                all_pred_list,\n","                valid_df,\n","            )\n","    # else:\n","    #     submitSeqSets[disc_type] = sequence_dataset(\n","    #         disc_type,\n","    #\n","    #         submit=True)\n","\n","print('Making validation sequence datasets...')\n","validSeqSets = manager.dict()\n","Parallel(n_jobs=-1, backend='multiprocessing')(\n","        delayed(generate_sequence_dataset)(disc_type, False)\n","       for disc_type in disc_type_to_ids\n","    )\n","print('Done.')\n","\n","# print('Making submit sequence datasets...')\n","# submitSeqSets = manager.dict()\n","# Parallel(n_jobs=-1, backend='multiprocessing')(\n","#         delayed(sequenceDataset)(disc_type, True)\n","#        for disc_type in disc_type_to_ids\n","#     )\n","# print('Done.')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\"\n","Re-Ranking Function\n","Downsample negative samples to 1:1 for efficiency/ease. There are many samples, and performance increase was observed.\n","\"\"\"\n","NEGATIVE_SAMPLE_RATIO = 10\n","\n","def resample(y):\n","    global resample_call\n","    counts = np.bincount(y)\n","    np.random.seed((resample_call+counts[0]) % 2**32)\n","\n","    neg_sample_count = NEGATIVE_SAMPLE_RATIO*counts[1]\n","    indices = np.concatenate((\n","        np.random.choice(np.arange(len(y))[y==0], neg_sample_count, replace=False),\n","        np.arange(len(y))[y==1]\n","    ))\n","    indices.sort()\n","    resample_call += 1\n","    return indices\n","\n","resample_call = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\"\n","Tune 3 Boosting Algorithm (XGBoost, LightGBM, CatBooost) with Cross Validation\n","Add more Classifier Algorithm\n","Original Source Code from:\n","    https://www.kaggle.com/code/chasembowers/sequence-postprocessing-v2-67-lb\n","\"\"\"\n","\n","NUM_FOLDS = 8\n","seq_cache = {} # For each fold and each text. cache score predictions sorted by score\n","clfs = []  # Each fold will add its classifier here\n","test_dataset = valid_df\n","train_text_df = pd.read_csv('/kaggle/input/fbp2-preprocessed-train-dataframe/final_converted_train_df.csv')\n","submitSeqSets = []\n","\n","\n","def xgb_predict_strings(\n","    disc_type: str,\n","    probThresh: float,\n","    test_groups,\n","    train_ind=None,\n","    submit=False\n","    ):\n","    \"\"\"\n","    Predict sub-sequences for a discourse type and set of train/test texts\n","    \"\"\"\n","    string_preds = []\n","    validSeqDs = validSeqSets[disc_type]\n","#    submitSeqDs = submitSeqSets[disc_type]\n","    \n","    # Average the probability predictions of a set of classifiers\n","    get_tp_prob = lambda testDs, classifiers: np.mean([clf.predict_proba(testDs.features)[:,1] for clf in classifiers], axis=0) if testDs.features.shape[0] > 0 else np.array([])\n","    \n","    \"\"\" Classifier Tuning workflow \"\"\"\n","    if not submit:  \n","        # Point to validation set values, tuned by validation dataset\n","        predict_df = test_dataset\n","        text_df = train_text_df\n","        groupIdx = np.isin(validSeqDs.groups, test_groups)\n","        testDs = SeqDataset(\n","            validSeqDs.features[groupIdx],\n","            validSeqDs.labels[groupIdx],\n","            validSeqDs.groups[groupIdx],\n","            validSeqDs.wordRanges[groupIdx],\n","            validSeqDs.truePos[groupIdx]\n","        )\n","        \n","        # Cache the classifier predictions to speed up tuning iterations\n","        seq_key = (\n","            disc_type,\n","            tuple(test_groups),\n","            tuple(train_ind)\n","        )\n","        if seq_key in seq_cache:\n","            text_to_seq = seq_cache[seq_key]\n","        else:\n","            clf = xgb.XGBClassifier(\n","                **CFG1.xgb_params\n","            )\n","            \n","            resampled = resample(validSeqDs.truePos[train_ind])\n","            clf.fit(\n","                validSeqDs.features[train_ind][resampled],\n","                validSeqDs.truePos[train_ind][resampled]\n","            )\n","            clfs.append(clf)\n","            prob_tp = get_tp_prob(testDs, [clf])\n","\n","    else:\n","        \"\"\" Making submission workflow \"\"\"\n","        # Point to submission set values\n","        predict_df = test_texts\n","        text_df = test_texts\n","        groupIdx = np.isin(submitSeqDs.groups, test_groups)\n","        testDs = SeqDataset(submitSeqDs.features[groupIdx], submitSeqDs.labels[groupIdx], submitSeqDs.groups[groupIdx], submitSeqDs.wordRanges[groupIdx], submitSeqDs.truePos[groupIdx])\n","        \n","        # Classifiers are always loaded from disc during submission\n","        with open( f\"../input/seqclassifiers6/{disc_type}_clf.p\", \"rb\" ) as clfFile:\n","            classifiers = pickle.load( clfFile )  \n","        prob_tp = get_tp_prob(testDs, classifiers)\n","    \n","    if submit or seq_key not in seq_cache:\n","        text_to_seq = {}\n","        for text_idx in test_groups:\n","            # The probability of true positive and (start,end) of each sub-sequence in the curent text\n","            prob_tp_curr = prob_tp[testDs.groups == text_idx]\n","            word_ranges_curr = testDs.wordRanges[testDs.groups == text_idx]\n","            sorted_seqs = list(reversed(sorted(zip(prob_tp_curr, [tuple(wr) for wr in word_ranges_curr]))))\n","            text_to_seq[text_idx] = sorted_seqs\n","        if not submit: \n","            seq_cache[seq_key] = text_to_seq\n","    \n","    for text_idx in test_groups:\n","        i = 1\n","        split_text = text_df.loc[text_df.id == predict_df.id.values[text_idx]].iloc[0].text.split()\n","        \n","        # Start and end word indices of sequence candidates kept in sorted order for efficiency\n","        starts = []\n","        ends = []\n","        \n","        # Include the sub-sequence predictions in order of predicted probability\n","        for prob, wordRange in text_to_seq[text_idx]:\n","            # Until the predicted probability is lower than the tuned threshold\n","            if prob < probThresh: \n","                break\n","                \n","            # Binary search already-placed word sequence intervals, and insert the new word sequence interval if it does not intersect an existing interval.\n","            insert = bisect_left(starts, wordRange[0])\n","            if (insert == 0 or ends[insert-1] <= wordRange[0]) and (insert == len(starts) or starts[insert] >= wordRange[1]):\n","                starts.insert(insert, wordRange[0])\n","                ends.insert(insert, wordRange[1])\n","                string_preds.append((predict_df.id.values[text_idx], disc_type, ' '.join(map(str, list(range(wordRange[0], wordRange[1]))))))\n","                i += 1     \n","    return string_preds\n","\n","def sub_df(string_preds):\n","    return pd.DataFrame(string_preds, columns=['id','class','predictionstring'])\n","    \n","# Convert skopt's uniform distribution over the tuning threshold to a distribution that exponentially decays from 100% to 0%\n","def prob_thresh(x): \n","    return .01*(100-np.exp(100*x))\n","\n","# Convert back to the scalar supplied by skopt\n","def skopt_thresh(x): \n","    return np.log((x/.01-100.)/-1.)/100.\n","    \n","def score_fmin(arr, disc_type):\n","    \"\"\"\n","    This function is called every tuning iteration.\n","    It takes the probability threshold as input and returns Macro F1\n","    \"\"\"\n","    validSeqDs = validSeqSets[disc_type]\n","    string_preds = []\n","    folds = np.array(list(GroupKFold(n_splits=NUM_FOLDS).split(validSeqDs.features, groups=validSeqDs.groups)))\n","    gt_indices = []\n","    for ind in folds[:,1]: gt_indices.extend(ind)\n","        \n","    # Texts that have no samples in our dataset for this class\n","    unsampled_texts = np.array(np.array_split(list(set(uniqueValidGroups).difference(set(np.unique(validSeqDs.groups)))), NUM_FOLDS))\n","    \n","    gt_texts = test_dataset.id.values[np.unique(validSeqDs.groups[np.array(gt_indices, dtype=int)]).astype(int)]\n","    \n","    # Generate predictions from each fold of the validation predictions\n","    for fold_i, (train_ind, test_ind) in enumerate(folds):\n","        string_preds.extend(\n","            xgb_predict_strings(\n","                disc_type, prob_thresh(arr[0]),\n","                np.concatenate((np.unique(validSeqDs.groups[test_ind]), unsampled_texts[fold_i])).astype(int),\n","                train_ind\n","            )\n","        )\n","    boost_df = sub_df(list(string_preds))\n","    gt_df = valid_df.loc[np.bitwise_and(valid_df['discourse_type']==disc_type, valid_df.id.isin(gt_texts))].copy()\n","    f1 = calculate_f1(boost_df.copy(), gt_df)\n","    return -f1\n","\n","def train_seq_clfs(disc_type):\n","    \"\"\" \n","    Function for Finding optimal Probability Threshold by using skopt\n","    function will find optimization bounds on the tuned probability threshold\n","    Source code from:\n","        https://www.kaggle.com/code/chasembowers/sequence-postprocessing-v2-67-lb\n","    \"\"\"\n","    space_start = skopt_thresh(.999)\n","    space_end = skopt_thresh(0)\n","    space  = [Real(space_start,space_end)]\n","    \n","    # Minimize F1\n","    score_fmin_disc = lambda arr: score_fmin(arr, disc_type)\n","    res_gp = gp_minimize(\n","        score_fmin_disc,\n","        space,\n","        n_calls=100,\n","        x0=[skopt_thresh(.5)]\n","    )\n","    \n","    # Use the gaussian approximation of f(threshold) -> F1 to select the minima\n","    thresh_cand = np.rot90([np.linspace(0,1,1000)])\n","    cand_scores = res_gp.models[-1].predict(thresh_cand)\n","    best_thresh_raw = space_start + (space_end - space_start)*thresh_cand[np.argmin(cand_scores)][0]\n","    best_thresh = prob_thresh(best_thresh_raw)\n","    exp_score = -np.min(cand_scores)\n","    \n","    # Make predictions at the inferred function minimum\n","    pred_thresh_score = -score_fmin_disc([best_thresh_raw])\n","    \n","    # And the best iteration in the optimization run\n","    best_iter_score = -score_fmin_disc(res_gp.x)\n","    \n","    # Save the trained classifiers to disc\n","    with open( f\"{disc_type}_clf.p\", \"wb\" ) as clfFile:\n","        pickle.dump( clfs, clfFile )\n","        \n","    # Save the tuning run results to file\n","    with open( f\"{disc_type}_res.p\", \"wb\" ) as resFile:\n","        pickle.dump( \n","            {\n","                'pred_thresh': best_thresh,  # The location of the minimum of the gaussian function inferred by skopt\n","                'min_thresh': prob_thresh(res_gp.x[0]),  # The threshold which produces the best score\n","                'pred_score': exp_score,  # The minimum of the gaussian function inferred by skopt\n","                'min_score': best_iter_score, # The best score in the tuning run\n","                'pred_thresh_score': pred_thresh_score  # The score produced by 'pred_thresh'\n","            }, \n","            resFile \n","        )\n","    print('Done training', disc_type)\n","    \n","if TRAIN_SEQ_CLASSIFIERS and not SUBMISSION:\n","    print('Training sequence classifiers... (This takes a long time.)')\n","    Parallel(n_jobs=-1, backend='multiprocessing')(\n","            delayed(train_seq_clfs)(disc_type) \n","           for disc_type in disc_type_to_ids\n","    )\n","    print('Done training all sequence classifiers.')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\" Check Tuning Result \"\"\"\n","thresholds = {}\n","for disc_type in disc_type_to_ids:\n","    with open( f\"/kaggle/working/{disc_type}_res.p\", \"rb\" ) as res_file:\n","        train_result = pickle.load( res_file )  \n","    thresholds[disc_type] = train_result['pred_thresh']\n","    print(f'{disc_type }: {train_result}', end='\\n\\n')\n","    \n","\"\"\" Submission Part \"\"\"\n","sub = pd.concat([sub_df(predict_strings(disc_type, thresholds[disc_type], uniqueSubmitGroups, submit=True)) for disc_type in disc_type_to_ids ]).reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
