{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## [Embedding Ensemble]  \n**1) Deit Cropped Dataset**  \n**2) YOLO V5 Cropped Dataset**  \n\n=> Weight Tuning by Optuna or Pytorch.lightening","metadata":{}},{"cell_type":"code","source":"!pip install faiss","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-13T13:03:01.782267Z","iopub.execute_input":"2023-04-13T13:03:01.782712Z","iopub.status.idle":"2023-04-13T13:03:13.514067Z","shell.execute_reply.started":"2023-04-13T13:03:01.782670Z","shell.execute_reply":"2023-04-13T13:03:13.512808Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting faiss\n  Downloading faiss-1.5.3-cp37-cp37m-manylinux1_x86_64.whl (4.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from faiss) (1.21.6)\nInstalling collected packages: faiss\nSuccessfully installed faiss-1.5.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os, sys, gc, time, random, warnings, math, cv2\nimport wandb, optuna, faiss, timm, torch\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport albumentations as albu\n\nfrom albumentations.pytorch import ToTensorV2\nfrom PIL import Image\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\nfrom timm.data.transforms_factory import create_transform\nfrom timm.optim import create_optimizer_v2\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import normalize\nfrom sklearn.preprocessing import LabelEncoder\nfrom kaggle_secrets import UserSecretsClient\nfrom glob import glob\nfrom pathlib import Path\nfrom typing import Callable, Dict, Optional, Tuple\nfrom torch.autograd import Variable\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-04-13T13:03:13.518475Z","iopub.execute_input":"2023-04-13T13:03:13.518806Z","iopub.status.idle":"2023-04-13T13:03:21.943862Z","shell.execute_reply.started":"2023-04-13T13:03:13.518754Z","shell.execute_reply":"2023-04-13T13:03:21.942766Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# WandB Login => Copy API Key\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb\")\n\n!wandb login $secret_value_0","metadata":{"execution":{"iopub.status.busy":"2023-04-13T13:03:21.945426Z","iopub.execute_input":"2023-04-13T13:03:21.946331Z","iopub.status.idle":"2023-04-13T13:03:25.914420Z","shell.execute_reply.started":"2023-04-13T13:03:21.946290Z","shell.execute_reply":"2023-04-13T13:03:25.913231Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}]},{"cell_type":"code","source":"class CFG:\n    checkpoint_dir = './saved/model'\n    name = 'HappyWhale'\n    model = 'convnext_base_384_in22ft1k'\n    resume_dir = '/kaggle/input/pytorch-arcface-train-with-focal-loss/conv-nextday322pl/convnext_base_384_in22ft1k_384.ckpt'\n\n    \"\"\" Common Options \"\"\"\n    wandb = True\n    optuna = True  # if you want to tune hyperparameter, set True\n    competition = 'HappyWhale'\n    seed = 42\n    cfg_name = 'CFG'\n    n_gpu = 1\n    \n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    gpu_id = 0\n    num_workers = 0\n\n    \"\"\" Data Options \"\"\"\n    n_folds = 5\n    epochs = 180\n    img_size = 384\n    batch_size = 64\n\n    \"\"\" Gradient Options \"\"\"\n    amp_scaler = True\n    gradient_checkpoint = True  # save parameter\n    clipping_grad = True  # clip_grad_norm\n    n_gradient_accumulation_steps = 1\n    max_grad_norm = 1000\n\n    \"\"\" Loss & Metrics Options \"\"\"\n    loss_fn = ''\n    reduction = 'mean'\n    metrics = ['MCRMSE', 'f_beta', 'recall']\n\n    \"\"\" Optimizer with LLRD Options \"\"\"\n    optimizer = 'AdamW'  # options: SWA, AdamW\n    llrd = True\n    layerwise_lr = 5e-5\n    layerwise_lr_decay = 0.9\n    layerwise_weight_decay = 1e-2\n    layerwise_adam_epsilon = 1e-6\n    layerwise_use_bertadam = False\n    betas = (0.9, 0.999)\n\n    \"\"\" Scheduler Options \"\"\"\n    scheduler = 'cosine_annealing'  # options: cosine, linear, cosine_annealing, linear_annealing\n    batch_scheduler = True\n    num_cycles = 0.5  # num_warmup_steps = 0\n    warmup_ratio = 0.1  # options: 0.05, 0.1\n\n    \"\"\" SWA Options \"\"\"\n    swa = True\n    swa_start = int(epochs*0.75)\n    swa_lr = 1e-4\n    anneal_epochs = 4\n    anneal_strategy = 'cos'  # default = cos, available option: linear\n\n    \"\"\" Model_Utils Options \"\"\"\n    freeze = False\n    reinit = True\n    num_reinit = 5\n    awp = False\n    nth_awp_start_epoch = 10\n    awp_eps = 1e-2\n    awp_lr = 1e-4","metadata":{"execution":{"iopub.status.busy":"2023-04-13T13:03:25.919229Z","iopub.execute_input":"2023-04-13T13:03:25.919543Z","iopub.status.idle":"2023-04-13T13:03:25.989669Z","shell.execute_reply.started":"2023-04-13T13:03:25.919512Z","shell.execute_reply":"2023-04-13T13:03:25.988525Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\"\"\" Helper Function \"\"\"\n\ndef check_device() -> bool:\n    return torch.mps.is_available()\n\ndef check_library(checker: bool) -> tuple:\n    \"\"\"\n    1) checker == True\n        - current device is mps\n    2) checker == False\n        - current device is cuda with cudnn\n    \"\"\"\n    if not checker:\n        _is_built = torch.backends.cudnn.is_available()\n        _is_enable = torch.backends.cudnn.enabledtorch.backends.cudnn.enabled\n        version = torch.backends.cudnn.version()\n        device = (_is_built, _is_enable, version)\n        return device\n\ndef class2dict(cfg) -> dict:\n    return dict((name, getattr(cfg, name)) for name in dir(cfg) if not name.startswith('__'))\n\n\ndef all_type_seed(cfg, checker: bool) -> None:\n    # python & torch seed\n    os.environ['PYTHONHASHSEED'] = str(cfg.seed)  # python Seed\n    random.seed(cfg.seed)  # random module Seed\n    np.random.seed(cfg.seed)  # numpy module Seed\n    torch.manual_seed(cfg.seed)  # Pytorch CPU Random Seed Maker\n\n    # device == cuda\n    if not checker:\n        torch.cuda.manual_seed(cfg.seed)  # Pytorch GPU Random Seed Maker\n        torch.cuda.manual_seed_all(cfg.seed)  # Pytorch Multi Core GPU Random Seed Maker\n        # torch.cudnn seed\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cudnn.enabled = False\n\ndef seed_worker(worker_id) -> None:\n    worker_seed = torch.initial_seed() % 2 ** 32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n    \n\ncheck_library(True)\nall_type_seed(CFG, True)\n\ng = torch.Generator()\ng.manual_seed(CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2023-04-13T13:03:25.991190Z","iopub.execute_input":"2023-04-13T13:03:25.991666Z","iopub.status.idle":"2023-04-13T13:03:26.012079Z","shell.execute_reply.started":"2023-04-13T13:03:25.991624Z","shell.execute_reply":"2023-04-13T13:03:26.011070Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7213d2175c30>"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\" Deit Cropped Dataset \"\"\"\n\"\"\" Path & Settings \"\"\"\n\nINPUT_DIR = Path(\"..\") / \"input\"\nOUTPUT_DIR = Path(\"/\") / \"kaggle\" / \"working\"\n\nDATA_ROOT_DIR = INPUT_DIR / \"convert-backfintfrecords\" / \"happy-whale-and-dolphin-backfin\"\nTRAIN_DIR = DATA_ROOT_DIR / \"train_images\"\nTEST_DIR = DATA_ROOT_DIR / \"test_images\"\nTRAIN_CSV_PATH = DATA_ROOT_DIR / \"train.csv\"\nSAMPLE_SUBMISSION_CSV_PATH = DATA_ROOT_DIR /\"sample_submission.csv\"\nPUBLIC_SUBMISSION_CSV_PATH = INPUT_DIR / \"whale-blender-v2-0\" / \"submission.csv\"\nIDS_WITHOUT_BACKFIN_PATH = INPUT_DIR / \"ids-without-backfin\" / \"ids_without_backfin.npy\"\n\nN_SPLITS = 5\n\n#ENCODER_CLASSES_PATH = OUTPUT_DIR /\" encoder_classes.npy\"\nTEST_CSV_PATH = OUTPUT_DIR / \" test.csv\"\n# TRAIN_CSV_ENCODED_FOLDED_PATH = OUTPUT_DIR / \"train_encoded_folded.csv\"\nCHECKPOINTS_DIR = OUTPUT_DIR / \"conv-nextday322pl\"\nSUBMISSION_CSV_PATH = OUTPUT_DIR / \"submission.csv\"\n\nDEBUG = False","metadata":{"execution":{"iopub.status.busy":"2023-04-13T13:03:26.013438Z","iopub.execute_input":"2023-04-13T13:03:26.014205Z","iopub.status.idle":"2023-04-13T13:03:26.021552Z","shell.execute_reply.started":"2023-04-13T13:03:26.014168Z","shell.execute_reply":"2023-04-13T13:03:26.020861Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\"\"\" YOLO V5 Cropped Dataset \"\"\"\n\"\"\" Path & Settings \"\"\"\n\n# INPUT_DIR = Path(\"..\") / \"input\"\n# OUTPUT_DIR = Path(\"/\") / \"kaggle\" / \"working\"\n\n# O_DATA_ROOT_DIR = INPUT_DIR / \"convert-backfintfrecords\" / \"happy-whale-and-dolphin-backfin\"\nsecond_DATA_ROOT_DIR = INPUT_DIR / \"/kaggle/input/happywhale-yolo5-cropped-dataset\"\nsecond_TRAIN_DIR = second_DATA_ROOT_DIR / \"train\" / \"train_images\"\nsecond_TEST_DIR = second_DATA_ROOT_DIR / \"test\" / \"test_images\"\nsecond_TRAIN_CSV_PATH = second_DATA_ROOT_DIR / \"train_df.csv\"\n# SAMPLE_SUBMISSION_CSV_PATH = O_DATA_ROOT_DIR /\"sample_submission.csv\"\n# PUBLIC_SUBMISSION_CSV_PATH = INPUT_DIR / \"whale-blender-v2-0\" / \"submission.csv\"\n# IDS_WITHOUT_BACKFIN_PATH = INPUT_DIR / \"ids-without-backfin\" / \"ids_without_backfin.npy\"\n\n# N_SPLITS = 5\n\nENCODER_CLASSES_PATH = '/kaggle/input/pytorch-arcface-train-with-focal-loss/ encoder_classes.npy'\nsecond_TEST_CSV_PATH = '/kaggle/input/pytorch-arcface-train-with-focal-loss/ test.csv'\nTRAIN_CSV_ENCODED_FOLDED_PATH = '/kaggle/input/pytorch-arcface-train-with-focal-loss/train_encoded_folded.csv'\n# CHECKPOINTS_DIR = OUTPUT_DIR / \"conv-nextday322pl\"\n# SUBMISSION_CSV_PATH = OUTPUT_DIR / \"submission.csv\"\n\n# DEBUG = False","metadata":{"execution":{"iopub.status.busy":"2023-04-13T13:03:26.023146Z","iopub.execute_input":"2023-04-13T13:03:26.024091Z","iopub.status.idle":"2023-04-13T13:03:26.034066Z","shell.execute_reply.started":"2023-04-13T13:03:26.024051Z","shell.execute_reply":"2023-04-13T13:03:26.033221Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def get_image_path(id: str, dir: Path) -> str:\n    return f\"{dir / id}\"","metadata":{"execution":{"iopub.status.busy":"2023-04-13T13:03:26.035614Z","iopub.execute_input":"2023-04-13T13:03:26.036516Z","iopub.status.idle":"2023-04-13T13:03:26.045488Z","shell.execute_reply.started":"2023-04-13T13:03:26.036479Z","shell.execute_reply":"2023-04-13T13:03:26.044817Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\"\"\" Make DataFrame & Cross Validation Function \"\"\"\n\ndef get_image_path(id: str, dir: Path) -> str:\n    return f\"{dir / id}\"\n\ndef stratifiedkfold(df: pd.DataFrame, cfg) -> pd.DataFrame:\n    \"\"\" Stratified KFold \"\"\"\n    fold = StratifiedKFold(\n        n_splits=cfg.n_folds,\n        shuffle=True,\n        random_state=cfg.seed\n    )\n    df['kfold'] = -1\n    for num, (tx, vx) in enumerate(fold.split(df, df.individual_id)):\n        df.loc[vx, \"kfold\"] = int(num)\n    return df\n\ndef load_data(data_path: str) -> pd.DataFrame:\n    \"\"\" Load data_folder from csv file like as train.csv, test.csv, val.csv \"\"\"\n    df = pd.read_csv(data_path)\n    df[\"image_path\"] = df[\"image\"].apply(get_image_path, dir=TRAIN_DIR)\n    return df\n\ndef img_preprocess(df: pd.DataFrame, cfg) -> pd.DataFrame:\n    \"\"\"\n    For Remove Background Image, Normalize Each Train & Test Data\n    \n    [Reference]\n    https://www.kaggle.com/code/remekkinas/remove-background-salient-object-detection/notebook\n    \"\"\"\n    encoder = LabelEncoder()\n    df[\"individual_id\"] = encoder.fit_transform(df[\"individual_id\"])\n    np.save(ENCODER_CLASSES_PATH, encoder.classes_)\n    df = stratifiedkfold(df, cfg)\n    df.to_csv(TRAIN_CSV_ENCODED_FOLDED_PATH, index=False)\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-04-13T13:03:26.046651Z","iopub.execute_input":"2023-04-13T13:03:26.046936Z","iopub.status.idle":"2023-04-13T13:03:26.058662Z","shell.execute_reply.started":"2023-04-13T13:03:26.046909Z","shell.execute_reply":"2023-04-13T13:03:26.057953Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\"\"\" load & Preprocess Train Data \"\"\"\n# train_df = img_preprocess(load_data(TRAIN_CSV_PATH), CFG)\n# train_df","metadata":{"execution":{"iopub.status.busy":"2023-04-13T13:03:26.063233Z","iopub.execute_input":"2023-04-13T13:03:26.063975Z","iopub.status.idle":"2023-04-13T13:03:26.072442Z","shell.execute_reply.started":"2023-04-13T13:03:26.063930Z","shell.execute_reply":"2023-04-13T13:03:26.071537Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"' load & Preprocess Train Data '"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\" For Deit Cropped Dataset \"\"\"\n# Use sample submission csv as template\ntest_df = pd.read_csv(SAMPLE_SUBMISSION_CSV_PATH)\ntest_df[\"image_path\"] = test_df[\"image\"].apply(get_image_path, dir=TEST_DIR)\n\ntest_df.drop(columns=[\"predictions\"], inplace=True)\n\n# Dummy id\ntest_df[\"individual_id\"] = 0\n\ntest_df.to_csv(TEST_CSV_PATH, index=False)\n\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-13T13:03:26.074330Z","iopub.execute_input":"2023-04-13T13:03:26.074769Z","iopub.status.idle":"2023-04-13T13:03:26.414809Z","shell.execute_reply.started":"2023-04-13T13:03:26.074733Z","shell.execute_reply":"2023-04-13T13:03:26.413803Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                image                                         image_path  \\\n0  000110707af0ba.jpg  ../input/convert-backfintfrecords/happy-whale-...   \n1  0006287ec424cb.jpg  ../input/convert-backfintfrecords/happy-whale-...   \n2  000809ecb2ccad.jpg  ../input/convert-backfintfrecords/happy-whale-...   \n3  00098d1376dab2.jpg  ../input/convert-backfintfrecords/happy-whale-...   \n4  000b8d89c738bd.jpg  ../input/convert-backfintfrecords/happy-whale-...   \n\n   individual_id  \n0              0  \n1              0  \n2              0  \n3              0  \n4              0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>image_path</th>\n      <th>individual_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000110707af0ba.jpg</td>\n      <td>../input/convert-backfintfrecords/happy-whale-...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0006287ec424cb.jpg</td>\n      <td>../input/convert-backfintfrecords/happy-whale-...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000809ecb2ccad.jpg</td>\n      <td>../input/convert-backfintfrecords/happy-whale-...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00098d1376dab2.jpg</td>\n      <td>../input/convert-backfintfrecords/happy-whale-...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>000b8d89c738bd.jpg</td>\n      <td>../input/convert-backfintfrecords/happy-whale-...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\" For YOLO V5 Cropped Dataset \"\"\"\nyolo_test_df = pd.read_csv('/kaggle/input/happywhale-yolo5-cropped-dataset/test_df.csv')\nyolo_test_df","metadata":{"execution":{"iopub.status.busy":"2023-04-13T13:03:26.418218Z","iopub.execute_input":"2023-04-13T13:03:26.418530Z","iopub.status.idle":"2023-04-13T13:03:26.496335Z","shell.execute_reply.started":"2023-04-13T13:03:26.418503Z","shell.execute_reply":"2023-04-13T13:03:26.495142Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                    image                                         image_path  \\\n0      000110707af0ba.jpg  ../input/happywhale-yolo5-cropped-dataset/test...   \n1      0006287ec424cb.jpg  ../input/happywhale-yolo5-cropped-dataset/test...   \n2      000809ecb2ccad.jpg  ../input/happywhale-yolo5-cropped-dataset/test...   \n3      00098d1376dab2.jpg  ../input/happywhale-yolo5-cropped-dataset/test...   \n4      000b8d89c738bd.jpg  ../input/happywhale-yolo5-cropped-dataset/test...   \n...                   ...                                                ...   \n27951  fff6ff1989b5cd.jpg  ../input/happywhale-yolo5-cropped-dataset/test...   \n27952  fff8fd932b42cb.jpg  ../input/happywhale-yolo5-cropped-dataset/test...   \n27953  fff96371332c16.jpg  ../input/happywhale-yolo5-cropped-dataset/test...   \n27954  fffc1c4d3eabc7.jpg  ../input/happywhale-yolo5-cropped-dataset/test...   \n27955  fffc50be10c175.jpg  ../input/happywhale-yolo5-cropped-dataset/test...   \n\n       individual_id  \n0                  0  \n1                  0  \n2                  0  \n3                  0  \n4                  0  \n...              ...  \n27951              0  \n27952              0  \n27953              0  \n27954              0  \n27955              0  \n\n[27956 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>image_path</th>\n      <th>individual_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000110707af0ba.jpg</td>\n      <td>../input/happywhale-yolo5-cropped-dataset/test...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0006287ec424cb.jpg</td>\n      <td>../input/happywhale-yolo5-cropped-dataset/test...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000809ecb2ccad.jpg</td>\n      <td>../input/happywhale-yolo5-cropped-dataset/test...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00098d1376dab2.jpg</td>\n      <td>../input/happywhale-yolo5-cropped-dataset/test...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>000b8d89c738bd.jpg</td>\n      <td>../input/happywhale-yolo5-cropped-dataset/test...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>27951</th>\n      <td>fff6ff1989b5cd.jpg</td>\n      <td>../input/happywhale-yolo5-cropped-dataset/test...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>27952</th>\n      <td>fff8fd932b42cb.jpg</td>\n      <td>../input/happywhale-yolo5-cropped-dataset/test...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>27953</th>\n      <td>fff96371332c16.jpg</td>\n      <td>../input/happywhale-yolo5-cropped-dataset/test...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>27954</th>\n      <td>fffc1c4d3eabc7.jpg</td>\n      <td>../input/happywhale-yolo5-cropped-dataset/test...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>27955</th>\n      <td>fffc50be10c175.jpg</td>\n      <td>../input/happywhale-yolo5-cropped-dataset/test...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>27956 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"class HappyWhaleDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, transform: Optional[Callable] = None):\n        self.df = df\n        self.transform = transform\n\n        self.image_names = self.df[\"image\"].values\n        self.image_paths = self.df[\"image_path\"].values\n        self.targets = self.df[\"individual_id\"].values\n\n    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n        image_name = self.image_names[index]\n\n        image_path = self.image_paths[index]\n\n        image = Image.open(image_path).convert(\"RGB\")\n        \n        if self.transform:\n            image = self.transform(image)\n\n        target = self.targets[index]\n        target = torch.tensor(target, dtype=torch.long)\n\n        return {\"image_name\": image_name, \"image\": image, \"target\": target}\n\n    def __len__(self) -> int:\n        return len(self.df)","metadata":{"execution":{"iopub.status.busy":"2023-04-13T13:03:26.498281Z","iopub.execute_input":"2023-04-13T13:03:26.498690Z","iopub.status.idle":"2023-04-13T13:03:26.509151Z","shell.execute_reply.started":"2023-04-13T13:03:26.498651Z","shell.execute_reply":"2023-04-13T13:03:26.507912Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class LitDataModule(pl.LightningDataModule):\n    def __init__(\n        self,\n        train_csv_encoded_folded: str,\n        test_csv: str,\n        val_fold: float,\n        image_size: int,\n        batch_size: int,\n        num_workers: int,\n    ):\n        super().__init__()\n\n        self.save_hyperparameters()\n\n        self.train_df = pd.read_csv(train_csv_encoded_folded)\n        self.test_df = pd.read_csv(test_csv)\n        \"\"\" Transform: \n            1) Resize\n            2) Normalize\n        \"\"\"\n        self.transform = create_transform(\n            input_size=(self.hparams.image_size, self.hparams.image_size),\n            crop_pct=1.0,\n        )\n        \n    def setup(self, stage: Optional[str] = None):\n        if stage == \"fit\" or stage is None:\n            # Split train df using fold\n            train_df = self.train_df[self.train_df.kfold != self.hparams.val_fold].reset_index(drop=True)\n            val_df = self.train_df[self.train_df.kfold == self.hparams.val_fold].reset_index(drop=True)\n\n            self.train_dataset = HappyWhaleDataset(train_df, transform=self.transform)\n            self.val_dataset = HappyWhaleDataset(val_df, transform=self.transform)\n\n        if stage == \"test\" or stage is None:\n            self.test_dataset = HappyWhaleDataset(self.test_df, transform=self.transform)\n\n    def train_dataloader(self) -> DataLoader:\n        return self._dataloader(self.train_dataset, train=True)\n\n    def val_dataloader(self) -> DataLoader:\n        return self._dataloader(self.val_dataset)\n\n    def test_dataloader(self) -> DataLoader:\n        return self._dataloader(self.test_dataset)\n\n    def _dataloader(self, dataset: HappyWhaleDataset, train: bool = False) -> DataLoader:\n        return DataLoader(\n            dataset,\n            batch_size=self.hparams.batch_size,\n            shuffle=train,\n            num_workers=self.hparams.num_workers,\n            pin_memory=True,\n            drop_last=train,\n        )","metadata":{"execution":{"iopub.status.busy":"2023-04-13T13:03:26.510971Z","iopub.execute_input":"2023-04-13T13:03:26.511561Z","iopub.status.idle":"2023-04-13T13:03:26.525718Z","shell.execute_reply.started":"2023-04-13T13:03:26.511520Z","shell.execute_reply":"2023-04-13T13:03:26.524758Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class WeightedLitDataModule(pl.LightningDataModule):\n    def __init__(\n        self,\n        train_csv_encoded_folded: str,\n        test_csv: str,\n        second_test_csv: str,\n        val_fold: float,\n        image_size: int,\n        batch_size: int,\n        num_workers: int,\n    ):\n        super().__init__()\n\n        self.save_hyperparameters()\n\n        self.train_df = pd.read_csv(train_csv_encoded_folded)\n        self.test_df = pd.read_csv(test_csv)\n        self.second_test_df = pd.read_csv(second_test_csv)\n        self.transform = create_transform(\n            input_size=(self.hparams.image_size, self.hparams.image_size),\n            crop_pct=1.0,\n        )\n        \n    def setup(self, stage: Optional[str] = None):\n        if stage == \"fit\" or stage is None:\n            # Split train df using fold\n            train_df = self.train_df[self.train_df.kfold != self.hparams.val_fold].reset_index(drop=True)\n            val_df = self.train_df[self.train_df.kfold == self.hparams.val_fold].reset_index(drop=True)\n\n            self.train_dataset = HappyWhaleDataset(train_df, transform=self.transform)\n            self.val_dataset = HappyWhaleDataset(val_df, transform=self.transform)\n\n        if stage == \"test\" or stage is None:\n            self.test_dataset = HappyWhaleDataset(self.test_df, transform=self.transform)\n            self.second_test_dataset = HappyWhaleDataset(self.second_test_df, transform=self.transform)\n\n    def train_dataloader(self) -> DataLoader:\n        return self._dataloader(self.train_dataset, train=True)\n\n    def val_dataloader(self) -> DataLoader:\n        return self._dataloader(self.val_dataset)\n\n    def test_dataloader(self) -> DataLoader:\n        return self._dataloader(self.test_dataset)\n    \n    def second_test_dataloader(self) -> DataLoader:\n        return self._dataloader(self.second_test_dataset)\n\n    def _dataloader(self, dataset: HappyWhaleDataset, train: bool = False) -> DataLoader:\n        return DataLoader(\n            dataset,\n            batch_size=self.hparams.batch_size,\n            shuffle=train,\n            num_workers=self.hparams.num_workers,\n            pin_memory=True,\n            drop_last=train,\n        )","metadata":{"execution":{"iopub.status.busy":"2023-04-13T13:03:26.527540Z","iopub.execute_input":"2023-04-13T13:03:26.527926Z","iopub.status.idle":"2023-04-13T13:03:26.544227Z","shell.execute_reply.started":"2023-04-13T13:03:26.527892Z","shell.execute_reply":"2023-04-13T13:03:26.543235Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# From https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/blob/master/src/modeling/metric_learning.py\n# Added type annotations, device, and 16bit support\nclass ArcMarginProduct(nn.Module):\n    r\"\"\"Implement of large margin arc distance: :\n    Args:\n        in_features: size of each input sample\n        out_features: size of each output sample\n        s: norm of input feature\n        m: margin\n        cos(theta + m)\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        s: float,\n        m: float,\n        easy_margin: bool,\n        ls_eps: float,\n    ):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features  # size of concat embedding size\n        self.out_features = out_features # num classes: \n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = math.cos(math.pi - m)\n        self.mm = math.sin(math.pi - m) * m\n\n    def forward(self, input: torch.Tensor, label: torch.Tensor, device: str = \"cuda\") -> torch.Tensor:\n        # --------------------------- cos(theta) & phi(theta) ---------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        # Enable 16 bit precision\n        cosine = cosine.to(torch.float32)\n\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device=device)\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) ------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-04-13T13:03:26.547302Z","iopub.execute_input":"2023-04-13T13:03:26.547924Z","iopub.status.idle":"2023-04-13T13:03:26.560498Z","shell.execute_reply.started":"2023-04-13T13:03:26.547887Z","shell.execute_reply":"2023-04-13T13:03:26.559436Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"\nfrom torch.autograd import Variable\n\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=0, alpha=None, size_average=True):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n        if isinstance(alpha, (float, int)): self.alpha = torch.Tensor([alpha, 1 - alpha])\n        if isinstance(alpha, list): self.alpha = torch.Tensor(alpha)\n        self.size_average = size_average\n\n    def forward(self, input, target):\n        if input.dim() > 2:\n            input = input.view(input.size(0), input.size(1), -1)  # N,C,H,W => N,C,H*W\n            input = input.transpose(1, 2)  # N,C,H*W => N,H*W,C\n            input = input.contiguous().view(-1, input.size(2))  # N,H*W,C => N*H*W,C\n        target = target.view(-1, 1)\n\n        logpt = F.log_softmax(input)\n        logpt = logpt.gather(1, target)\n        logpt = logpt.view(-1)\n        pt = Variable(logpt.data.exp())\n\n        if self.alpha is not None:\n            if self.alpha.type() != input.data.type():\n                self.alpha = self.alpha.type_as(input.data)\n            select = (target != 0).type(torch.LongTensor).cuda()\n            at = self.alpha.gather(0, select.data.view(-1))\n            logpt = logpt * Variable(at)\n\n        loss = -1 * (1 - pt) ** self.gamma * logpt\n        if self.size_average:\n            return loss.mean()\n        else:\n            return loss.sum()\n\n\nclass SwitchNorm1d(nn.Module):\n    def __init__(self, num_features, eps=1e-5, momentum=0.997, using_moving_average=True):\n        super(SwitchNorm1d, self).__init__()\n        self.eps = eps\n        self.momentum = momentum\n        self.using_moving_average = using_moving_average\n        self.weight = nn.Parameter(torch.ones(1, num_features))\n        self.bias = nn.Parameter(torch.zeros(1, num_features))\n        self.mean_weight = nn.Parameter(torch.ones(2))\n        self.var_weight = nn.Parameter(torch.ones(2))\n        self.register_buffer('running_mean', torch.zeros(1, num_features))\n        self.register_buffer('running_var', torch.zeros(1, num_features))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.running_mean.zero_()\n        self.running_var.zero_()\n        self.weight.data.fill_(1)\n        self.bias.data.zero_()\n\n    def _check_input_dim(self, input):\n        if input.dim() != 2:\n            raise ValueError('expected 2D input (got {}D input)'\n                             .format(input.dim()))\n\n    def forward(self, x):\n        self._check_input_dim(x)\n        mean_ln = x.mean(1, keepdim=True)\n        var_ln = x.var(1, keepdim=True)\n\n        if self.training:\n            mean_bn = x.mean(0, keepdim=True)\n            var_bn = x.var(0, keepdim=True)\n            if self.using_moving_average:\n                self.running_mean.mul_(self.momentum)\n                self.running_mean.add_((1 - self.momentum) * mean_bn.data)\n                self.running_var.mul_(self.momentum)\n                self.running_var.add_((1 - self.momentum) * var_bn.data)\n            else:\n                self.running_mean.add_(mean_bn.data)\n                self.running_var.add_(mean_bn.data ** 2 + var_bn.data)\n        else:\n            mean_bn = torch.autograd.Variable(self.running_mean)\n            var_bn = torch.autograd.Variable(self.running_var)\n\n        softmax = nn.Softmax(0)\n        mean_weight = softmax(self.mean_weight)\n        var_weight = softmax(self.var_weight)\n\n        mean = mean_weight[0] * mean_ln + mean_weight[1] * mean_bn\n        var = var_weight[0] * var_ln + var_weight[1] * var_bn\n\n        x = (x - mean) / (var + self.eps).sqrt()\n        return x * self.weight + self.bias\n\n\nclass LitModule(pl.LightningModule):\n    def __init__(\n        self,\n        model_name: str,\n        pretrained: bool,\n        drop_rate: float,\n        embedding_size: int,\n        num_classes: int,\n        arc_s: float,\n        arc_m: float,\n        arc_easy_margin: bool,\n        arc_ls_eps: float,\n        optimizer: str,\n        learning_rate: float,\n        weight_decay: float,\n        len_train_dl: int,\n        epochs:int\n    ):\n        super().__init__()\n\n        self.save_hyperparameters()\n          # self.fea_extra_layer = [2, 3]\n        self.fea_extra_layer = [-2,-1]\n        self.model = timm.create_model(model_name, pretrained=False, drop_rate=drop_rate,\n                                       features_only=True,\n                                       out_indices=self.fea_extra_layer\n                                       )\n        in_features = 1536\n        self.embedding = nn.Sequential(\n\n            nn.Linear(in_features, embedding_size),\n            nn.BatchNorm1d(embedding_size))\n\n        # self.model.reset_classifier(num_classes=0, global_pool=\"avg\")\n        self.bn = nn.Sequential(\n            nn.BatchNorm2d(1024),\n            nn.Dropout(0.2),\n            nn.AdaptiveAvgPool2d(1),\n        )\n        self.bn2 = nn.Sequential(\n            nn.BatchNorm2d(512),\n            nn.Dropout(0.2),\n            nn.AdaptiveAvgPool2d(1),\n        )\n\n        self.arc = ArcMarginProduct(\n            in_features=embedding_size,\n            out_features=num_classes,\n            s=arc_s,\n            m=arc_m,\n            easy_margin=arc_easy_margin,\n            ls_eps=arc_ls_eps,\n        )\n        self.loss_fn = nn.CrossEntropyLoss()\n\n    def forward(self, images: torch.Tensor) -> torch.Tensor:\n        features = self.model(images)\n        features[0] = self.bn2(features[0])\n        features[1] = self.bn(features[1])\n        features = torch.cat(features, dim=1)\n        embeddings = self.embedding(features.flatten(1))\n\n        return embeddings\n\n    def configure_optimizers(self):\n        optimizer = create_optimizer_v2(\n            self.parameters(),\n            opt=self.hparams.optimizer,\n            lr=self.hparams.learning_rate,\n            weight_decay=self.hparams.weight_decay,\n        )\n        \n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            optimizer,\n            self.hparams.learning_rate,\n            steps_per_epoch=self.hparams.len_train_dl,\n            epochs=self.hparams.epochs,\n        )\n        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\"}\n\n        return [optimizer], [scheduler]\n\n    def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n        return self._step(batch, \"train\")\n\n    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n        return self._step(batch, \"val\")\n\n    def _step(self, batch: Dict[str, torch.Tensor], step: str) -> torch.Tensor:\n        images, targets = batch[\"image\"], batch[\"target\"]\n\n        embeddings = self(images)\n        outputs = self.arc(embeddings, targets, self.device)\n\n        loss = self.loss_fn(outputs, targets)\n        \n        self.log(f\"{step}_loss\", loss)\n\n        return loss","metadata":{"execution":{"iopub.status.busy":"2023-04-13T13:03:26.562311Z","iopub.execute_input":"2023-04-13T13:03:26.562677Z","iopub.status.idle":"2023-04-13T13:03:26.596404Z","shell.execute_reply.started":"2023-04-13T13:03:26.562640Z","shell.execute_reply":"2023-04-13T13:03:26.595251Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def load_eval_module(checkpoint_path: str, device: torch.device) -> LitModule:\n    module = LitModule.load_from_checkpoint(checkpoint_path)\n    module.to(device)\n    module.eval()\n\n    return module\n\ndef load_dataloaders(\n    train_csv_encoded_folded: str,\n    test_csv: str,\n    val_fold: float,\n    image_size: int,\n    batch_size: int,\n    num_workers: int,\n) -> Tuple[DataLoader, DataLoader, DataLoader]:\n\n    datamodule = LitDataModule(\n        train_csv_encoded_folded=train_csv_encoded_folded,\n        test_csv=test_csv,\n        val_fold=val_fold,\n        image_size=image_size,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        \n    )\n\n    datamodule.setup()\n\n    train_dl = datamodule.train_dataloader()\n    val_dl = datamodule.val_dataloader()\n    test_dl = datamodule.test_dataloader()\n\n    return train_dl, val_dl, test_dl\n\ndef weighted_load_dataloaders(\n    train_csv_encoded_folded: str,\n    test_csv: str,\n    second_test_csv: str,\n    val_fold: float,\n    image_size: int,\n    batch_size: int,\n    num_workers: int,\n) -> Tuple[DataLoader, DataLoader, DataLoader, DataLoader]:\n\n    datamodule = WeightedLitDataModule(\n        train_csv_encoded_folded=train_csv_encoded_folded,\n        test_csv=test_csv,\n        second_test_csv=second_test_csv,\n        val_fold=val_fold,\n        image_size=image_size,\n        batch_size=batch_size,\n        num_workers=num_workers,\n    )\n\n    datamodule.setup()\n\n    train_dl = datamodule.train_dataloader()\n    val_dl = datamodule.val_dataloader()\n    test_dl = datamodule.test_dataloader()\n    second_test_dl = datamodule.second_test_dataloader()\n\n    return train_dl, val_dl, test_dl, second_test_dl\n\n\ndef load_encoder() -> LabelEncoder:\n    encoder = LabelEncoder()\n    encoder.classes_ = np.load(ENCODER_CLASSES_PATH, allow_pickle=True)\n\n    return encoder\n\n\n@torch.inference_mode()\ndef get_embeddings(\n    module: pl.LightningModule, dataloader: DataLoader, encoder: LabelEncoder, stage: str\n) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n\n    all_image_names = []\n    all_embeddings = []\n    all_targets = []\n\n    for batch in tqdm(dataloader, desc=f\"Creating {stage} embeddings\"):\n        image_names = batch[\"image_name\"]\n        images = batch[\"image\"].to(module.device)\n        targets = batch[\"target\"].to(module.device)\n\n        embeddings = module(images)\n        #emb2 = module2(images)\n        #embeddings = embeddings + emb2   \n        all_image_names.append(image_names)\n        all_embeddings.append(embeddings.cpu().numpy())\n        all_targets.append(targets.cpu().numpy())\n        \n        if DEBUG:\n            break\n\n    all_image_names = np.concatenate(all_image_names)\n    all_embeddings = np.vstack(all_embeddings)\n    all_targets = np.concatenate(all_targets)\n\n    all_embeddings = normalize(all_embeddings, axis=1, norm=\"l2\")\n    all_targets = encoder.inverse_transform(all_targets)\n\n    return all_image_names, all_embeddings, all_targets\n\n\ndef create_and_search_index(embedding_size: int, train_embeddings: np.ndarray, val_embeddings: np.ndarray, k: int):\n    index = faiss.IndexFlatIP(embedding_size)\n    index.add(train_embeddings)\n    D, I = index.search(val_embeddings, k=k)  # noqa: E741\n\n    return D, I\n\n\ndef create_val_targets_df(\n    train_targets: np.ndarray, val_image_names: np.ndarray, val_targets: np.ndarray\n) -> pd.DataFrame:\n\n    allowed_targets = np.unique(train_targets)\n    val_targets_df = pd.DataFrame(np.stack([val_image_names, val_targets], axis=1), columns=[\"image\", \"target\"])\n    val_targets_df.loc[~val_targets_df.target.isin(allowed_targets), \"target\"] = \"new_individual\"\n\n    return val_targets_df\n\n\ndef create_distances_df(\n    image_names: np.ndarray, targets: np.ndarray, D: np.ndarray, I: np.ndarray, stage: str  # noqa: E741\n) -> pd.DataFrame:\n\n    distances_df = []\n    for i, image_name in tqdm(enumerate(image_names), desc=f\"Creating {stage}_df\"):\n        target = targets[I[i]]\n        distances = D[i]\n        subset_preds = pd.DataFrame(np.stack([target, distances], axis=1), columns=[\"target\", \"distances\"])\n        subset_preds[\"image\"] = image_name\n        distances_df.append(subset_preds)\n\n    distances_df = pd.concat(distances_df).reset_index(drop=True)\n    distances_df = distances_df.groupby([\"image\", \"target\"]).distances.max().reset_index()\n    distances_df = distances_df.sort_values(\"distances\", ascending=False).reset_index(drop=True)\n\n    return distances_df\n\n\ndef get_best_threshold(val_targets_df: pd.DataFrame, valid_df: pd.DataFrame) -> Tuple[float, float]:\n    best_th = 0\n    best_cv = 0\n    for th in [0.1 * x for x in range(11)]:\n        all_preds = get_predictions(valid_df, threshold=th)\n\n        cv = 0\n        for i, row in val_targets_df.iterrows():\n            target = row.target\n            preds = all_preds[row.image]\n            val_targets_df.loc[i, th] = map_per_image(target, preds)\n\n        cv = val_targets_df[th].mean()\n\n        print(f\"th={th} cv={cv}\")\n\n        if cv > best_cv:\n            best_th = th\n            best_cv = cv\n\n    print(f\"best_th={best_th}\")\n    print(f\"best_cv={best_cv}\")\n\n    # Adjustment: Since Public lb has nearly 10% 'new_individual' (Be Careful for private LB)\n    val_targets_df[\"is_new_individual\"] = val_targets_df.target == \"new_individual\"\n    val_scores = val_targets_df.groupby(\"is_new_individual\").mean().T\n    val_scores[\"adjusted_cv\"] = val_scores[True] * 0.1 + val_scores[False] * 0.9\n    best_th = val_scores[\"adjusted_cv\"].idxmax()\n    print(f\"best_th_adjusted={best_th}\")\n\n    return best_th, best_cv\n\n\ndef get_predictions(df: pd.DataFrame, threshold: float = 0.2):\n    sample_list = [\"938b7e931166\", \"5bf17305f073\", \"7593d2aee842\", \"7362d7a01d00\", \"956562ff2888\"]\n\n    predictions = {}\n    for i, row in tqdm(df.iterrows(), total=len(df), desc=f\"Creating predictions for threshold={threshold}\"):\n        if row.image in predictions:\n            if len(predictions[row.image]) == 5:\n                continue\n            predictions[row.image].append(row.target)\n        elif row.distances > threshold:\n            predictions[row.image] = [row.target, \"new_individual\"]\n        else:\n            predictions[row.image] = [\"new_individual\", row.target]\n\n    for x in tqdm(predictions):\n        if len(predictions[x]) < 5:\n            remaining = [y for y in sample_list if y not in predictions]\n            predictions[x] = predictions[x] + remaining\n            predictions[x] = predictions[x][:5]\n\n    return predictions\n\n\n# TODO: add types\n# mAP\ndef map_per_image(label, predictions):\n    \"\"\"Computes the precision score of one image.\n\n    Parameters\n    ----------\n    label : string\n            The true label of the image\n    predictions : list\n            A list of predicted elements (order does matter, 5 predictions allowed per image)\n\n    Returns\n    -------\n    score : double\n    \"\"\"\n    try:\n        return 1 / (predictions[:5].index(label) + 1)\n    except ValueError:\n        return 0.0\n\n\ndef create_predictions_df(test_df: pd.DataFrame, best_th: float) -> pd.DataFrame:\n    predictions = get_predictions(test_df, best_th)\n\n    predictions = pd.Series(predictions).reset_index()\n    predictions.columns = [\"image\", \"predictions\"]\n    predictions[\"predictions\"] = predictions[\"predictions\"].apply(lambda x: \" \".join(x))\n\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2023-04-13T13:03:26.599378Z","iopub.execute_input":"2023-04-13T13:03:26.600047Z","iopub.status.idle":"2023-04-13T13:03:26.631546Z","shell.execute_reply.started":"2023-04-13T13:03:26.600010Z","shell.execute_reply":"2023-04-13T13:03:26.630269Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"\"\"\" 여기 함수를 루프로 감싸버리자, 그리고 임베딩 추출 뒷 부분부터는 그냥 함수 밖으로 빼서 한번에 퉁치기 \"\"\"\n\"\"\" \nArgs:\n    checkpoint_path: Fine-Tuned Model by Deit Cropped & YOLO V5 Cropped\n    train_csv_encoded_folded: Whole Dataset (About 90000+) which is Deit Cropped & YOLO V5 Cropped\n    test_csv: Each Dataset's Test DataFrame\n\"\"\"\ndef weighted_infer(\n    checkpoint_path: str,\n    train_csv_encoded_folded: str = str(TRAIN_CSV_ENCODED_FOLDED_PATH),\n    test_csv: str = str(TEST_CSV_PATH),\n    second_test_csv: str = str(second_TEST_CSV_PATH),\n    val_fold: float = 0.0,\n    image_size: int = 256,\n    batch_size: int = 32,\n    num_workers: int = 2,\n    k: int = 50,\n):\n    new_embeddings = []\n    ids_without_backfin = np.load(IDS_WITHOUT_BACKFIN_PATH, allow_pickle=True) # 얘가 지금 임베딩 비는 애들\n    module = load_eval_module(checkpoint_path, torch.device(\"cuda\"))\n    #module2 = load_eval_module(checkpoint_path2, torch.device(\"cuda\"))\n    train_dl, val_dl, test_dl, second_test_dl = weighted_load_dataloaders(\n        train_csv_encoded_folded=train_csv_encoded_folded,\n        test_csv=test_csv,\n        second_test_csv=second_test_csv,\n        val_fold=val_fold,\n        image_size=image_size,\n        batch_size=batch_size,\n        num_workers=num_workers,\n    )\n\n    encoder = load_encoder()\n    test_image_names, test_embeddings, test_targets = get_embeddings(module, test_dl, encoder, stage=\"test\")\n    print(test_embeddings, test_embeddings.shape)\n\n    second_test_image_names, second_test_embeddings, second_test_targets = get_embeddings(module, second_test_dl, encoder, stage=\"test\")\n    print(second_test_embeddings, test_embeddings.shape)\n\n    for idx in range(len(test_image_names)):\n        if test_image_names[idx] in ids_without_backfin:\n            new_embeddings.append(second_test_embeddings[idx])\n        else:\n            new_embeddings.append(0.5 * np.add(test_embeddings[idx], second_test_embeddings[idx]))\n    \n    test_embeddings = np.array(new_embeddings)\n    \n    del second_test_image_names, second_test_targets, second_test_embeddings\n    gc.collect()\n    torch.cuda.empty_cache()\n        \n    D, I = create_and_search_index(module.hparams.embedding_size, train_embeddings, val_embeddings, k)  # noqa: E741\n    print(\"Created index with train_embeddings\")\n\n    val_targets_df = create_val_targets_df(train_targets, val_image_names, val_targets)\n    print(f\"val_targets_df=\\n{val_targets_df.head()}\")\n\n    val_df = create_distances_df(val_image_names, train_targets, D, I, \"val\")\n    print(f\"val_df=\\n{val_df.head()}\")\n\n    best_th, best_cv = get_best_threshold(val_targets_df, val_df)\n    print(f\"val_targets_df=\\n{val_targets_df.describe()}\")\n\n    train_embeddings = np.concatenate([train_embeddings, val_embeddings]) # train dataset's embedding\n    train_targets = np.concatenate([train_targets, val_targets]) # train dataset's embedding\n    print(\"Updated train_embeddings and train_targets with val data\")\n\n    D, I = create_and_search_index(module.hparams.embedding_size, train_embeddings, test_embeddings, k)  # 여기 test_embeddings를 1+2로 하면 되겠네\n    print(\"Created index with train_embeddings\")\n\n    test_df = create_distances_df(test_image_names, train_targets, D, I, \"test\")\n    print(f\"test_df=\\n{test_df.head()}\")\n    predictions = create_predictions_df(test_df, best_th)\n    print(f\"predictions.head()={predictions.head()}\")\n    \n    # Fix missing predictions\n    public_predictions = pd.read_csv(PUBLIC_SUBMISSION_CSV_PATH)\n    # ids_without_backfin = np.load(IDS_WITHOUT_BACKFIN_PATH, allow_pickle=True) # 얘가 지금 임베딩 비는 애들\n\n    ids2 = public_predictions[\"image\"][~public_predictions[\"image\"].isin(predictions[\"image\"])]\n    predictions = pd.concat(\n        [\n            predictions[~(predictions[\"image\"].isin(ids_without_backfin))],\n            public_predictions[public_predictions[\"image\"].isin(ids_without_backfin)],\n            public_predictions[public_predictions[\"image\"].isin(ids2)],\n        ]\n    )\n    predictions = predictions.drop_duplicates()\n\n    predictions.to_csv(SUBMISSION_CSV_PATH, index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-13T13:42:00.834461Z","iopub.execute_input":"2023-04-13T13:42:00.834861Z","iopub.status.idle":"2023-04-13T13:42:00.853579Z","shell.execute_reply.started":"2023-04-13T13:42:00.834822Z","shell.execute_reply":"2023-04-13T13:42:00.852538Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"image_size = 384\nbatch_size = 32\nweighted_infer(\n    checkpoint_path=f\"{CFG.resume_dir}\",\n    train_csv_encoded_folded = str(TRAIN_CSV_ENCODED_FOLDED_PATH),\n    test_csv = str(TEST_CSV_PATH),\n    second_test_csv = str(second_TEST_CSV_PATH),\n    image_size=image_size, batch_size=batch_size,\n    k=100\n)","metadata":{"execution":{"iopub.status.busy":"2023-04-13T13:42:05.894843Z","iopub.execute_input":"2023-04-13T13:42:05.895903Z","iopub.status.idle":"2023-04-13T13:59:58.369635Z","shell.execute_reply.started":"2023-04-13T13:42:05.895824Z","shell.execute_reply":"2023-04-13T13:59:58.368118Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"Creating test embeddings:   0%|          | 0/874 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ef99dabe77046e6a9aac140edf907d8"}},"metadata":{}},{"name":"stdout","text":"[[ 0.07979687  0.06576561 -0.02406937 ...  0.03794983 -0.02177413\n  -0.01135578]\n [-0.02665154  0.053927    0.03176337 ... -0.0439869  -0.01988871\n  -0.04037159]\n [-0.05702118  0.03273489  0.0507337  ...  0.03958606 -0.04100629\n   0.10138851]\n ...\n [-0.06211325  0.08108444  0.02756541 ...  0.02824374  0.04517635\n  -0.11374559]\n [-0.00908647 -0.01007448 -0.02222575 ...  0.032947    0.02258123\n   0.0343171 ]\n [-0.01046135 -0.00119797 -0.00544814 ... -0.04452109 -0.04238594\n   0.01275638]] (27942, 512)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Creating test embeddings:   0%|          | 0/874 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a60221683f24ddda5bf79b0261d9e26"}},"metadata":{}},{"name":"stdout","text":"[[ 0.07979687  0.06576561 -0.02406937 ...  0.03794983 -0.02177413\n  -0.01135578]\n [-0.02665154  0.053927    0.03176337 ... -0.0439869  -0.01988871\n  -0.04037159]\n [-0.05702118  0.03273489  0.0507337  ...  0.03958606 -0.04100629\n   0.10138851]\n ...\n [-0.06211325  0.08108444  0.02756541 ...  0.02824374  0.04517635\n  -0.11374559]\n [-0.00908647 -0.01007448 -0.02222575 ...  0.032947    0.02258123\n   0.0343171 ]\n [-0.01046135 -0.00119797 -0.00544814 ... -0.04452109 -0.04238594\n   0.01275638]] (27942, 512)\n(27942, 512)\n['000110707af0ba.jpg' '0006287ec424cb.jpg' '000809ecb2ccad.jpg' ...\n 'fff96371332c16.jpg' 'fffc1c4d3eabc7.jpg' 'fffc50be10c175.jpg'] (27942,)\n[[ 0.07979687  0.06576561 -0.02406937 ...  0.03794983 -0.02177413\n  -0.01135578]\n [-0.02665154  0.053927    0.03176337 ... -0.0439869  -0.01988871\n  -0.04037159]\n [-0.05702118  0.03273489  0.0507337  ...  0.03958606 -0.04100629\n   0.10138851]\n ...\n [-0.06211325  0.08108444  0.02756541 ...  0.02824374  0.04517635\n  -0.11374559]\n [-0.00908647 -0.01007448 -0.02222575 ...  0.032947    0.02258123\n   0.0343171 ]\n [-0.01046135 -0.00119797 -0.00544814 ... -0.04452109 -0.04238594\n   0.01275638]] (27942, 512)\n['0013f1f5f2f0' '0013f1f5f2f0' '0013f1f5f2f0' ... '0013f1f5f2f0'\n '0013f1f5f2f0' '0013f1f5f2f0'] (27942,)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/408550387.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msecond_test_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecond_TEST_CSV_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n","\u001b[0;32m/tmp/ipykernel_23/1041213688.py\u001b[0m in \u001b[0;36mweighted_infer\u001b[0;34m(checkpoint_path, train_csv_encoded_folded, test_csv, second_test_csv, val_fold, image_size, batch_size, num_workers, k)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m#test_embeddings = np.mean(test_embeddings+ test_embeddings2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_and_search_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: E741\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Created index with train_embeddings\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'train_embeddings' referenced before assignment"],"ename":"UnboundLocalError","evalue":"local variable 'train_embeddings' referenced before assignment","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}